{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_segmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnl/XHs1HD3/QSxfN2bap4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/gan_segmentation_FE/blob/main/gan_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB9IbZ_eJD7p",
        "outputId": "bdb2fa59-4d8d-451a-e99e-5c4f028c5632"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOaLqvOCAbvx",
        "outputId": "6dc6594c-a375-4030-a2df-6bfeff8349c7"
      },
      "source": [
        "%%writefile util.py\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "def string_tensor_from_idlist_and_path(type, name = None):\n",
        "  file_names = glob.glob('/content/gdrive/MyDrive/2dsegmentation/'+type+'_*.npy')\n",
        "  string_tensor = tf.Variable(file_names, trainable=False,\n",
        "                              name=name, validate_shape=False)\n",
        "  return string_tensor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting util.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkkiaIo-Aukg",
        "outputId": "2116b9b4-13c9-4a76-8348-a0faa74aaa7c"
      },
      "source": [
        "!pip install -U tensorflow==1.13.0rc1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow==1.13.0rc1 in /usr/local/lib/python3.7/dist-packages (1.13.0rc1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.12.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0rc1) (4.0.3)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.0rc1) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.0rc1) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMJm1QSr_Yma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dae76e-d478-4168-ec80-37cfd0aa97dc"
      },
      "source": [
        "!pip install -U scipy==1.2.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scipy==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bldAuwzxAeE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06142e56-8c38-4d51-e6d1-2e4b30d09b5f"
      },
      "source": [
        "%%writefile load_folder_images.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "def _decode_and_preprocess_image(image_file, shift_param = -128, rescale_param = 128, resized_image_size = [128, 128]):\n",
        "  image = tf.image.decode_png(image_file);\n",
        "  \n",
        "  #image.set_shape((32, 32, 3))\n",
        "  \n",
        "  \n",
        "  image = tf.cast(image, tf.float32)\n",
        "  \n",
        "  if shift_param != 0:\n",
        "    image = tf.add(image, shift_param)\n",
        "  \n",
        "  if rescale_param != 0:\n",
        "    image = tf.multiply(image, 1.0/rescale_param)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #image = tf.random_crop(image, [32, 32, 3])\n",
        "  image = tf.image.resize_images(image, resized_image_size) \n",
        "  image.set_shape((resized_image_size[0], resized_image_size[1], 1))\n",
        "  #image = tf.image.grayscasle_to_rgb(image, 'torgb')\n",
        "  return image\n",
        "\n",
        "\n",
        "def _load_images(image_file, batch_size, num_preprocess_threads, min_queue_examples, shift_param = -128, rescale_param = 128, resized_image_size = [128, 128], shuffle = True):\n",
        "  \n",
        "  image = _decode_and_preprocess_image(image_file, shift_param, rescale_param, resized_image_size)\n",
        "  #image = tf.image.grayscasle_to_rgb(image, 'torgb')\n",
        "  images = []\n",
        "  if shuffle == True:\n",
        "    images = tf.train.shuffle_batch(\n",
        "          [image],\n",
        "          batch_size=batch_size,\n",
        "          num_threads=num_preprocess_threads,\n",
        "          capacity=min_queue_examples + 3 * batch_size,\n",
        "          min_after_dequeue=min_queue_examples)\n",
        "  else:\n",
        "    images = tf.train.batch(\n",
        "          [image],\n",
        "          batch_size=batch_size,\n",
        "          num_threads=num_preprocess_threads,\n",
        "          capacity=min_queue_examples + 3 * batch_size)\n",
        "  \n",
        "  \n",
        "  return images\n",
        "\n",
        "def load_image_and_segmentation_from_idlist(idlist_tensor_img, idlist_tensor_seg, batch_size, num_preprocess_threads, min_queue_examples, shift_params = [-128, -0.5], rescale_params = [128, 0.5], resized_image_size = [128, 128], shuffle = True):\n",
        "\n",
        "  filename_queue = glob.glob('/content/gdrive/MyDrive/2dsegmentation/'+'image'+'_*.npy')\n",
        "  image_file_image=[]\n",
        "  for path in filename_queue:\n",
        "    image_file_image.append(np.expand_dims(np.resize(np.load(path),(128,128)),-1))\n",
        "  image_file_image=np.stack(image_file_image,0)\n",
        "  image_file_image[image_file_image>250]=250\n",
        "  image_file_image[image_file_image<-200]=-200\n",
        "  image_file_image=(image_file_image+185.5479 )/56.159664\n",
        "  processed_image=tf.convert_to_tensor(image_file_image)\n",
        "  processed_image = tf.cast(processed_image, tf.float32)\n",
        "\n",
        "  filename_queue = glob.glob('/content/gdrive/MyDrive/2dsegmentation/'+'segmentation'+'_*.npy')\n",
        "  image_file_seg=[]\n",
        "  for path in filename_queue:\n",
        "    image_file_seg.append(np.expand_dims(np.resize(np.load(path),(128,128)),-1))\n",
        "  image_file_seg=np.stack(image_file_seg,0)\n",
        "  image_file_seg=(image_file_seg-0.00015476771763392856)/0.012439604679711764\n",
        "  processed_seg=tf.convert_to_tensor(image_file_seg)\n",
        "  processed_seg = tf.cast(processed_seg, tf.float32)\n",
        "  if shuffle == True:\n",
        "    images, segmentations = tf.train.shuffle_batch(\n",
        "          [processed_image, processed_seg],\n",
        "          batch_size=batch_size, enqueue_many=True,\n",
        "          num_threads=num_preprocess_threads,\n",
        "          capacity=min_queue_examples + 3 * batch_size,\n",
        "          min_after_dequeue=min_queue_examples)\n",
        "  else:\n",
        "    images, segmentations = tf.train.batch(\n",
        "          [processed_image, processed_seg],\n",
        "          batch_size=batch_size, enqueue_many=True,\n",
        "          num_threads=num_preprocess_threads,\n",
        "          capacity=min_queue_examples + 3 * batch_size)\n",
        "    \n",
        "  return images, segmentations\n",
        "  \n",
        "\n",
        "def load_images_from_idlist(idlist, batch_size, num_preprocess_threads, min_queue_examples, shift_param = -128, rescale_param = 128, resized_image_size = [128, 128], shuffle = True):\n",
        "  # Make a queue of file names including all the image files in the relative\n",
        "  # image directory.\n",
        "  filename_queue = tf.train.string_input_producer(idlist,\n",
        "                                                  shuffle=shuffle)\n",
        "  \n",
        "  # Read an entire image file. If the images\n",
        "  # are too large they could be split in advance to smaller files or use the Fixed\n",
        "  # reader to split up the file.\n",
        "  image_reader = tf.WholeFileReader()\n",
        "  \n",
        "  # Read a whole file from the queue, the first returned value in the tuple is the\n",
        "  # filename which we are ignoring.\n",
        "  _, image_file = image_reader.read(filename_queue)\n",
        "\n",
        "  return _load_images(image_file, batch_size, num_preprocess_threads, min_queue_examples, shift_param, rescale_param, resized_image_size, shuffle)\n",
        "\n",
        "def load_images(folder_path_match, batch_size, num_preprocess_threads, min_queue_examples, shift_param = -128, rescale_param = 128, resized_image_size = [128, 128], shuffle = True):\n",
        "  # Make a queue of file names including all the image files in the relative\n",
        "  # image directory.\n",
        "  filename_queue = tf.train.string_input_producer(\n",
        "    tf.train.match_filenames_once(folder_path_match),\n",
        "    shuffle=shuffle)\n",
        "  \n",
        "  # Read an entire image file. If the images\n",
        "  # are too large they could be split in advance to smaller files or use the Fixed\n",
        "  # reader to split up the file.\n",
        "  image_reader = tf.WholeFileReader()\n",
        "  \n",
        "  # Read a whole file from the queue, the first returned value in the tuple is the\n",
        "  # filename which we are ignoring.\n",
        "  _, image_file = image_reader.read(filename_queue)\n",
        "\n",
        "  return _load_images(image_file, batch_size, num_preprocess_threads, min_queue_examples, shift_param, rescale_param, resized_image_size, shuffle)\n",
        "\n",
        "def load_image(image_path, num_preprocess_threads, min_queue_examples, resized_image_size = [64, 64]):\n",
        "  # Make a queue of file names including all the image files in the relative\n",
        "  # image directory.\n",
        "  filename_queue = tf.train.string_input_producer(\n",
        "    tf.train.match_filenames_once(image_path))\n",
        "  \n",
        "  # Read an entire image file. If the images\n",
        "  # are too large they could be split in advance to smaller files or use the Fixed\n",
        "  # reader to split up the file.\n",
        "  image_reader = tf.WholeFileReader()\n",
        "  \n",
        "  # Read a whole file from the queue, the first returned value in the tuple is the\n",
        "  # filename which we are ignoring.\n",
        "  _, image_file = image_reader.read(filename_queue)\n",
        "  \n",
        "  image = tf.image.decode_png(image_file);\n",
        "  \n",
        "  #image.set_shape((32, 32, 3))\n",
        "  \n",
        "  \n",
        "  image = tf.cast(image, tf.float32)\n",
        "  \n",
        "  image = tf.add(image, -128)\n",
        "  image = tf.multiply(image, 1.0/128)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #image = tf.random_crop(image, [32, 32, 3])\n",
        "  image = tf.image.resize_images(image, resized_image_size) \n",
        "  image.set_shape((resized_image_size[0], resized_image_size[1], 1))\n",
        "  #image = tf.image.grayscasle_to_rgb(image, 'torgb')\n",
        "  \n",
        "  images = tf.train.shuffle_batch(\n",
        "      [image],\n",
        "      batch_size=1,\n",
        "      num_threads=num_preprocess_threads,\n",
        "      capacity=min_queue_examples + 3 * 1,\n",
        "      min_after_dequeue=min_queue_examples)\n",
        "  \n",
        "  \n",
        "  \n",
        "  return images"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting load_folder_images.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGP_XNCYAl8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd8b3c9-ba0e-4078-cabb-f744b36d48cb"
      },
      "source": [
        "%%writefile dcgan.py\n",
        "import tensorflow as tf\n",
        "\n",
        "def linear(input_tensor, output_dim, scope=None, stddev=1.0):\n",
        "    norm = tf.random_normal_initializer(stddev=stddev)\n",
        "    const = tf.constant_initializer(0.0)\n",
        "    with tf.variable_scope(scope or 'linear'):\n",
        "        w = tf.get_variable('w', [input_tensor.get_shape()[1], output_dim], initializer=norm)\n",
        "        b = tf.get_variable('b', [output_dim], initializer=const)\n",
        "        return tf.matmul(input_tensor, w) + b\n",
        "      \n",
        "def minibatch(input_tensor, num_kernels=256, kernel_dim=4):\n",
        "        in_vec = tf.reshape(input_tensor, [input_tensor.get_shape().as_list()[0], -1])\n",
        "        x = linear(in_vec, num_kernels * kernel_dim, scope='minibatch', stddev=0.02)\n",
        "        activation = tf.reshape(x, (-1, num_kernels, kernel_dim))\n",
        "        diffs = tf.expand_dims(activation, 3) - tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\n",
        "        abs_diffs = tf.reduce_sum(tf.abs(diffs), 2)\n",
        "        minibatch_features = tf.reduce_sum(tf.exp(-abs_diffs), 2)\n",
        "        #minibatch_features = tf.reshape(minibatch_features, (128, 4, 4, -1))\n",
        "        test_out = tf.concat(axis=1, values=[in_vec, minibatch_features])\n",
        "        test_out = tf.reshape(test_out, [input_tensor.get_shape().as_list()[0], input_tensor.get_shape().as_list()[1], input_tensor.get_shape().as_list()[2], -1])\n",
        "        return test_out\n",
        "\n",
        "def disc_conv2d(input_tensor, num_inputs, num_outputs, kernel_size, stride, dropout_ratio, scope):\n",
        "  with tf.variable_scope(scope):\n",
        "              w = tf.get_variable(\n",
        "                        'w',\n",
        "                        [kernel_size, kernel_size, num_inputs, num_outputs],\n",
        "                        tf.float32,\n",
        "                        tf.truncated_normal_initializer(stddev=0.05))\n",
        "              b = tf.get_variable(\n",
        "                  'b',\n",
        "                  [num_outputs],\n",
        "                  tf.float32,\n",
        "                  tf.zeros_initializer())\n",
        "              c = tf.nn.conv2d(input_tensor, w, [1, stride, stride, 1], 'SAME')\n",
        "              mean, variance = tf.nn.moments(c, [0, 1, 2])\n",
        "              outputs = leaky_relu(tf.nn.batch_normalization(c, mean, variance, b, None, 1e-5))\n",
        "              #outputs = tf.nn.dropout(outputs, 0.5)\n",
        "              return outputs;\n",
        "              #out.append(outputs)\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, depths=[1024, 512, 256, 128, 64], f_size=4):\n",
        "        self.reuse = False\n",
        "        self.f_size = f_size\n",
        "        self.depths = depths + [2]\n",
        "\n",
        "    def model(self, inputs):\n",
        "        i_depth = self.depths[0:(len(self.depths) - 1)]\n",
        "        o_depth = self.depths[1:(len(self.depths))]\n",
        "        out = []\n",
        "        \n",
        "        with tf.variable_scope('g', reuse=self.reuse):\n",
        "            # reshape from inputs\n",
        "            inputs = tf.convert_to_tensor(inputs)\n",
        "            with tf.variable_scope('fc_reshape'):\n",
        "                w0 = tf.get_variable(\n",
        "                    'w',\n",
        "                    [inputs.get_shape()[-1], i_depth[0] * self.f_size * self.f_size],\n",
        "                    tf.float32,\n",
        "                    tf.truncated_normal_initializer(stddev=0.05))\n",
        "                b0 = tf.get_variable(\n",
        "                    'b',\n",
        "                    [i_depth[0]],\n",
        "                    tf.float32,\n",
        "                    tf.zeros_initializer())\n",
        "                fc = tf.matmul(inputs, w0)\n",
        "                reshaped = tf.reshape(fc, [-1, self.f_size, self.f_size, i_depth[0]])\n",
        "                mean, variance = tf.nn.moments(reshaped, [0, 1, 2])\n",
        "                outputs = tf.nn.relu(tf.nn.batch_normalization(reshaped, mean, variance, b0, None, 1e-5))\n",
        "                out.append(outputs)\n",
        "            # deconvolution (transpose of convolution) x 4\n",
        "            \n",
        "            \n",
        "            for i in range(len(self.depths) - 1):\n",
        "                    \n",
        "                with tf.variable_scope('conv%d' % (i + 1)):\n",
        "                    kernel_size = 5\n",
        "    \n",
        "                     \n",
        "                    w = tf.get_variable(\n",
        "                        'w',\n",
        "                        [kernel_size, kernel_size, o_depth[i], i_depth[i]],\n",
        "                        tf.float32,\n",
        "                        tf.truncated_normal_initializer(stddev=0.05))\n",
        "                    b = tf.get_variable(\n",
        "                        'b',\n",
        "                        [o_depth[i]],\n",
        "                        tf.float32,\n",
        "                        tf.zeros_initializer())\n",
        "                    dc = tf.nn.conv2d_transpose(\n",
        "                        outputs,\n",
        "                        w,\n",
        "                        [\n",
        "                            int(outputs.get_shape()[0]),\n",
        "                            self.f_size * 2 ** (i + 1),\n",
        "                            self.f_size * 2 ** (i + 1),\n",
        "                            o_depth[i]\n",
        "                        ],\n",
        "                        [1, 2, 2, 1])\n",
        "                    if i < len(self.depths) - 2:\n",
        "                        mean, variance = tf.nn.moments(dc, [0, 1, 2])\n",
        "                        outputs = tf.nn.relu(tf.nn.batch_normalization(dc, mean, variance, b, None, 1e-5))\n",
        "                    else:\n",
        "                        outputs = tf.nn.bias_add(dc, b)\n",
        "                        outputs = tf.nn.tanh(outputs)\n",
        "                    out.append(outputs)\n",
        "                    \n",
        "        \n",
        "        self.reuse = True\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g')\n",
        "        return out\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "\n",
        "class Discriminator:\n",
        "    def __init__(self, depths=[64, 128, 256, 512]):\n",
        "        self.reuse = False\n",
        "        self.depths = [2] + depths\n",
        "        \n",
        "\n",
        "\n",
        "    def model(self, inputs, return_feature_vector=False):\n",
        "        def leaky_relu(x, leak=0.2):\n",
        "            return tf.maximum(x, x * leak)\n",
        "        i_depth = self.depths[0:(len(self.depths) - 1)]\n",
        "        o_depth = self.depths[1:(len(self.depths))]\n",
        "        out = []\n",
        "        self.weights = []\n",
        "        with tf.variable_scope('d', reuse=self.reuse):\n",
        "            outputs = inputs\n",
        "            # convolution x 4\n",
        "\n",
        "            for i in range(len(self.depths) - 1):\n",
        "                with tf.variable_scope('conv%d' % i):\n",
        "                    w = tf.get_variable(\n",
        "                        'w',\n",
        "                        [5, 5, outputs.get_shape()[3], o_depth[i]],\n",
        "                        tf.float32,\n",
        "                        tf.truncated_normal_initializer(stddev=0.05))\n",
        "                    b = tf.get_variable(\n",
        "                        'b',\n",
        "                        [o_depth[i]],\n",
        "                        tf.float32,\n",
        "                        tf.zeros_initializer())\n",
        "                    c = tf.nn.conv2d(outputs, w, [1, 2, 2, 1], 'SAME')\n",
        "                    mean, variance = tf.nn.moments(c, [0, 1, 2])\n",
        "                    outputs = leaky_relu(tf.nn.batch_normalization(c, mean, variance, b, None, 1e-5))\n",
        "                    \n",
        "                    self.weights.append(w);\n",
        "                    \n",
        "                    out.append(outputs)\n",
        "                    \n",
        "            if(return_feature_vector == True):\n",
        "                return out[3]      \n",
        "            \n",
        "            with tf.variable_scope('classify'):\n",
        "                dim = 1\n",
        "                for d in outputs.get_shape()[1:].as_list():\n",
        "                    dim *= d\n",
        "                    print(dim)\n",
        "                w = tf.get_variable('w', [dim, 2], tf.float32, tf.truncated_normal_initializer(stddev=0.02))\n",
        "                b = tf.get_variable('b', [2], tf.float32, tf.zeros_initializer())\n",
        "                out.append(tf.nn.bias_add(tf.matmul(tf.reshape(outputs, [-1, dim]), w), b))\n",
        "        self.reuse = True\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d')\n",
        "        return out\n",
        "\n",
        "    def __call__(self, inputs, return_feature_vector=False):\n",
        "        return self.model(inputs, return_feature_vector)\n",
        "\n",
        "\n",
        "class DCGAN:\n",
        "    def __init__(self,\n",
        "                 batch_size=128, f_size=2, z_dim=400,\n",
        "                 gdepth1=512, gdepth2=256, gdepth3=128, gdepth4=128,\n",
        "                 ddepth1=128,   ddepth2=128, ddepth3=256, ddepth4=512):\n",
        "        self.batch_size = batch_size\n",
        "        self.f_size = f_size\n",
        "        self.z_dim = z_dim\n",
        "        self.g = Generator(depths=[gdepth1, gdepth2, gdepth3, gdepth4, gdepth4, gdepth4], f_size=self.f_size)\n",
        "        self.d = Discriminator(depths=[ddepth1, ddepth2, ddepth3, ddepth4, ddepth4, ddepth4])\n",
        "        self.z = tf.random_uniform([self.batch_size, self.z_dim], minval=-1.0, maxval=1.0)\n",
        "        self.losses = {\n",
        "            'g': None,\n",
        "            'd': None\n",
        "        }\n",
        "\n",
        "    def build(self, input_images,\n",
        "              learning_rate=0.0004, beta1=0.5, feature_matching=False):\n",
        "        \"\"\"build model, generate losses, train op\"\"\"\n",
        "        generated_images = self.g(self.z)[-1]\n",
        "        print('before d gen')\n",
        "        \n",
        "        outputs_from_g = self.d(generated_images)\n",
        "        \n",
        "        print('before d real')\n",
        "        outputs_from_i = self.d(input_images)\n",
        "        logits_from_g = outputs_from_g[-1]\n",
        "        logits_from_i = outputs_from_i[-1]\n",
        "  \n",
        "        tf.add_to_collection(\n",
        "            'g_losses',\n",
        "            tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=logits_from_g, labels=tf.ones([self.batch_size], dtype=tf.int64))))\n",
        "        \n",
        "        \n",
        "        d_loss_real = tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                     logits=logits_from_i, labels=tf.ones([self.batch_size], dtype=tf.int64)))\n",
        "        \n",
        "        d_loss_fake = tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=logits_from_g, labels=tf.zeros([self.batch_size], dtype=tf.int64)))\n",
        "        \n",
        "        tf.add_to_collection(\n",
        "             'd_losses', d_loss_real\n",
        "             )\n",
        "        tf.add_to_collection(\n",
        "            'd_losses',d_loss_fake\n",
        "            )\n",
        "        \n",
        " \n",
        "        \n",
        "        if feature_matching:\n",
        "            features_from_g = tf.reduce_mean(outputs_from_g[-2], axis=(0))\n",
        "            features_from_i = tf.reduce_mean(outputs_from_i[-2], axis=(0))\n",
        "            tf.add_to_collection('g_losses', tf.multiply(tf.nn.l2_loss(features_from_g - features_from_i), 0.1))\n",
        "            mean_image_from_g = tf.reduce_mean(generated_images, axis=(0))\n",
        "            mean_image_from_i = tf.reduce_mean(input_images, axis=(0))\n",
        "            tf.add_to_collection('g_losses', tf.multiply(tf.nn.l2_loss(mean_image_from_g - mean_image_from_i), 0.01))\n",
        "\n",
        "        self.losses['g'] = tf.add_n(tf.get_collection('g_losses'), name='total_g_loss')\n",
        "        self.losses['d'] = tf.add_n(tf.get_collection('d_losses'), name='total_d_loss')\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
        "        d_opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
        "\n",
        "        self.g_opt_op = g_opt.minimize(self.losses['g'], var_list=self.g.variables)\n",
        "        self.d_opt_op = d_opt.minimize(self.losses['d'], var_list=self.d.variables)\n",
        "        \n",
        "        with tf.control_dependencies([self.g_opt_op, self.d_opt_op]):\n",
        "            self.train = tf.no_op(name='train')\n",
        "        return self.train\n",
        "\n",
        "    def sample_images(self, row=8, col=8, num_samples = None, inputs=None):\n",
        "        if inputs is None:\n",
        "            inputs = self.z\n",
        "            \n",
        "        if num_samples is None:\n",
        "            num_samples = self.batch_size\n",
        "            \n",
        "        inputs = tf.random_uniform([num_samples, self.z_dim], minval=-1.0, maxval=1.0)\n",
        "        images = tf.cast(tf.multiply(tf.add(self.g(inputs)[-1], 1.0), 127.5), tf.uint8)\n",
        "          \n",
        "        return images\n",
        "        images = [image for image in tf.split(axis=0, num_or_size_splits=self.batch_size, value=images)]\n",
        "        rows = []\n",
        "        for i in range(row):\n",
        "            rows.append(tf.concat(axis=2, values=images[col * i + 0:col * i + col]))\n",
        "        image = tf.concat(axis=1, values=rows)\n",
        "        return tf.image.encode_jpeg(tf.squeeze(image, [0]))\n",
        "      \n",
        "    def retrieve_feature_vector(self, inputs):\n",
        "        vector = tf.reshape(self.d(inputs, return_feature_vector=True), [-1]) \n",
        "        return vector"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting dcgan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqF-AasuAp37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d288ed43-3c57-4fa0-e6f3-f5fa32913eb4"
      },
      "source": [
        "%%writefile train.py\n",
        "from datetime import datetime\n",
        "import time\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "from dcgan import *\n",
        "import numpy as np \n",
        "from scipy.misc import imsave\n",
        "import load_folder_images\n",
        "import os\n",
        "from load_folder_images import load_image_and_segmentation_from_idlist\n",
        "from load_folder_images import load_image\n",
        "import csv\n",
        "import util\n",
        "\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_string('train_dir', '/tmp/dcgan_train',\n",
        "                           \"\"\"Directory where to write event logs \"\"\"\n",
        "                           \"\"\"and checkpoint.\"\"\")\n",
        "tf.app.flags.DEFINE_integer('max_steps', 100000,\n",
        "                            \"\"\"Number of batches to run.\"\"\")\n",
        "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
        "                            \"\"\"Whether to log device placement.\"\"\")\n",
        "\n",
        "tf.app.flags.DEFINE_string(\"working_directory\", \"working_dir\", \"\")\n",
        "tf.app.flags.DEFINE_string(\"checkpoint_dir\", \"checkpoints\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "z_dim = 100\n",
        "batch_size = 4\n",
        "learning_rate = 0.0004\n",
        "beta1 = 0.5\n",
        "monitoring_batches = 70; #number of batches after which some samples are saved\n",
        "num_monitoring_cycles = 100;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "  idlist_img_name = \"list_of_img_ids.txt\"\n",
        "  idlist_seg_name = \"list_of_seg_ids.txt\"\n",
        "  img_folder_path = \"image_folder_path/\"\n",
        "  seg_folder_path = \"segmentation_folder_path/\"\n",
        "\n",
        "  idlist_tensor_img = util.string_tensor_from_idlist_and_path('image')\n",
        "  idlist_tensor_seg = util.string_tensor_from_idlist_and_path('segmentation')\n",
        "  \n",
        "  \n",
        "\n",
        "  images, segmentation_images = load_image_and_segmentation_from_idlist(idlist_tensor_img, idlist_tensor_seg, batch_size, 16, 2560, shift_params = [-128, -0.5], rescale_params = [128, 0.5], shuffle = True)\n",
        "  print(images.shape, segmentation_images.shape)\n",
        "  \n",
        "  dcgan = DCGAN(batch_size)\n",
        "  input_images = images\n",
        "  \n",
        "  input_plus_segmentation = tf.concat([input_images, segmentation_images], 3)\n",
        "  print(input_plus_segmentation.shape)\n",
        "  train_op = dcgan.build(input_plus_segmentation)\n",
        "  print('here')\n",
        "  sample_images = dcgan.sample_images()\n",
        "  print('here')  \n",
        "  saver = tf.train.Saver(max_to_keep = 50)\n",
        "  print('here')\n",
        "\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "      summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=sess.graph)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      \n",
        "      \n",
        "      coord = tf.train.Coordinator()\n",
        "      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "      \n",
        "      total_iters_max =  num_monitoring_cycles * monitoring_batches\n",
        "      iters = 0\n",
        "      total_duration = 0.0\n",
        "      avg_iter_time = 0.0\n",
        "      \n",
        "      for cycle in range(num_monitoring_cycles):\n",
        "        for batch in range(monitoring_batches):\n",
        "          start_time = time.time()\n",
        "          _, g_loss_value, d_loss_value = sess.run([train_op, dcgan.losses['g'], dcgan.losses['d']])\n",
        "          _, g_loss_value, d_loss_value = sess.run([dcgan.g_opt_op, dcgan.losses['g'], dcgan.losses['d']])        \n",
        "            \n",
        "          duration = time.time() - start_time\n",
        "          total_duration = total_duration + duration\n",
        "          iters = iters + 1\n",
        "          avg_iter_time = (avg_iter_time * (iters - 1) + duration) / iters\n",
        "          #ETA_seconds = (total_iters_max / iters) * total_duration - total_duration\n",
        "          ETA_seconds = (0.95 * avg_iter_time + 0.05 * duration) * (total_iters_max - iters)\n",
        "          format_str = 'cycle (%d / %d), batch (%d / %d) loss = (G: %.8f, D: %.8f) (%.3f sec/batch) (ETA: %d seconds)'\n",
        "          print(format_str % (cycle, num_monitoring_cycles, batch, monitoring_batches, g_loss_value, d_loss_value, duration, int(ETA_seconds)))\n",
        "        \n",
        "        \n",
        "        checkpoint_folder = FLAGS.working_directory + \"/\" + FLAGS.checkpoint_dir + '/checkpoint%d.ckpt' % cycle\n",
        "        if not os.path.exists(checkpoint_folder):\n",
        "          os.makedirs(checkpoint_folder)\n",
        "        saver.save(sess, checkpoint_folder)\n",
        "\n",
        "\n",
        "        imgs = sess.run(sample_images)\n",
        "         \n",
        "        for k in range(batch_size):\n",
        "            imgs_folder = os.path.join(FLAGS.working_directory, 'out/imgs%d/') % cycle\n",
        "            if not os.path.exists(imgs_folder):\n",
        "              os.makedirs(imgs_folder)\n",
        "              \n",
        "            segs_folder = os.path.join(FLAGS.working_directory, 'out/segs%d/') % cycle\n",
        "            if not os.path.exists(segs_folder):\n",
        "              os.makedirs(segs_folder)\n",
        "              \n",
        "            img_channel = imgs[k][:, :, 0]\n",
        "            img_seg = imgs[k][:, :, 1]\n",
        "            imsave(os.path.join(imgs_folder, 'img_%d.png') % k,\n",
        "                     img_channel.reshape(128, 128))\n",
        "            imsave(os.path.join(segs_folder, 'img_%d.png') % k,\n",
        "                     img_seg.reshape(128, 128))\n",
        "              \n",
        "        imgs_in = sess.run(input_plus_segmentation)    \n",
        "        for k in range(batch_size):\n",
        "          imgs_folder = os.path.join(FLAGS.working_directory, 'in/imgs%d/') % cycle\n",
        "          if not os.path.exists(imgs_folder):\n",
        "            os.makedirs(imgs_folder)\n",
        "            \n",
        "          segs_folder = os.path.join(FLAGS.working_directory, 'in/segs%d/') % cycle\n",
        "          if not os.path.exists(segs_folder):\n",
        "            os.makedirs(segs_folder)\n",
        "\n",
        "          img_channel = imgs_in[k][:, :, 0]\n",
        "          img_seg = imgs_in[k][:, :, 1]\n",
        "          imsave(os.path.join(imgs_folder, 'img_%d.png') % k,\n",
        "                     img_channel.reshape(128, 128))\n",
        "          imsave(os.path.join(segs_folder, 'img_%d.png') % k,\n",
        "                     img_seg.reshape(128, 128))\n",
        "              \n",
        "      coord.request_stop()\n",
        "      coord.join(threads)\n",
        "      \n",
        "     \n",
        "\n",
        "def main(argv=None):  # pylint: disable=unused-argument\n",
        "\n",
        "  \n",
        "  if tf.gfile.Exists(FLAGS.train_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.train_dir)\n",
        "  train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.app.run()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrmRMNsFCFOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d02708-1622-4fe7-c6a9-105df20963a3"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/load_folder_images.py:77: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:861: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "(4, 128, 128, 1) (4, 128, 128, 1)\n",
            "(4, 128, 128, 2)\n",
            "before d gen\n",
            "2\n",
            "4\n",
            "2048\n",
            "before d real\n",
            "2\n",
            "4\n",
            "2048\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "here\n",
            "here\n",
            "here\n",
            "2021-05-16 18:19:40.608395: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-05-16 18:19:40.611547: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-05-16 18:19:40.611772: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x555ec2e431e0 executing computations on platform Host. Devices:\n",
            "2021-05-16 18:19:40.611803: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From train.py:76: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "cycle (0 / 100), batch (0 / 70) loss = (G: 3.87283373, D: 1.19988215) (5.547 sec/batch) (ETA: 38821 seconds)\n",
            "cycle (0 / 100), batch (1 / 70) loss = (G: 1.69071448, D: 0.45218602) (3.603 sec/batch) (ETA: 31676 seconds)\n",
            "cycle (0 / 100), batch (2 / 70) loss = (G: 4.54022026, D: 0.28453580) (3.567 sec/batch) (ETA: 29426 seconds)\n",
            "cycle (0 / 100), batch (3 / 70) loss = (G: 4.30499935, D: 0.22185844) (3.578 sec/batch) (ETA: 28326 seconds)\n",
            "cycle (0 / 100), batch (4 / 70) loss = (G: 3.52643418, D: 0.28394011) (3.608 sec/batch) (ETA: 27714 seconds)\n",
            "cycle (0 / 100), batch (5 / 70) loss = (G: 4.55425501, D: 0.89988202) (3.616 sec/batch) (ETA: 27309 seconds)\n",
            "cycle (0 / 100), batch (6 / 70) loss = (G: 8.46562481, D: 1.31376135) (3.604 sec/batch) (ETA: 27002 seconds)\n",
            "cycle (0 / 100), batch (7 / 70) loss = (G: 4.79171848, D: 0.59015042) (3.542 sec/batch) (ETA: 26700 seconds)\n",
            "cycle (0 / 100), batch (8 / 70) loss = (G: 10.43629837, D: 0.91468346) (3.569 sec/batch) (ETA: 26511 seconds)\n",
            "cycle (0 / 100), batch (9 / 70) loss = (G: 7.26373148, D: 0.04045683) (3.581 sec/batch) (ETA: 26363 seconds)\n",
            "cycle (0 / 100), batch (10 / 70) loss = (G: 1.67249513, D: 0.27081895) (3.536 sec/batch) (ETA: 26195 seconds)\n",
            "cycle (0 / 100), batch (11 / 70) loss = (G: 9.94203568, D: 0.12074672) (3.473 sec/batch) (ETA: 26012 seconds)\n",
            "cycle (0 / 100), batch (12 / 70) loss = (G: 9.87515640, D: 0.40242347) (3.442 sec/batch) (ETA: 25847 seconds)\n",
            "cycle (0 / 100), batch (13 / 70) loss = (G: 6.97489119, D: 0.11625452) (3.429 sec/batch) (ETA: 25704 seconds)\n",
            "cycle (0 / 100), batch (14 / 70) loss = (G: 1.87941360, D: 0.35448197) (3.455 sec/batch) (ETA: 25604 seconds)\n",
            "cycle (0 / 100), batch (15 / 70) loss = (G: 7.07069016, D: 0.15020101) (3.523 sec/batch) (ETA: 25561 seconds)\n",
            "cycle (0 / 100), batch (16 / 70) loss = (G: 7.35058784, D: 0.11831150) (3.526 sec/batch) (ETA: 25503 seconds)\n",
            "cycle (0 / 100), batch (17 / 70) loss = (G: 4.22935677, D: 0.23831987) (3.494 sec/batch) (ETA: 25428 seconds)\n",
            "cycle (0 / 100), batch (18 / 70) loss = (G: 1.14307547, D: 0.43631676) (3.462 sec/batch) (ETA: 25347 seconds)\n",
            "cycle (0 / 100), batch (19 / 70) loss = (G: 8.64661598, D: 0.19970338) (3.495 sec/batch) (ETA: 25307 seconds)\n",
            "cycle (0 / 100), batch (20 / 70) loss = (G: 9.04304314, D: 2.04243636) (3.511 sec/batch) (ETA: 25271 seconds)\n",
            "cycle (0 / 100), batch (21 / 70) loss = (G: 7.58474159, D: 0.24979837) (3.466 sec/batch) (ETA: 25203 seconds)\n",
            "cycle (0 / 100), batch (22 / 70) loss = (G: 4.60494232, D: 0.06358669) (3.522 sec/batch) (ETA: 25191 seconds)\n",
            "cycle (0 / 100), batch (23 / 70) loss = (G: 0.93867749, D: 0.62786955) (3.564 sec/batch) (ETA: 25188 seconds)\n",
            "cycle (0 / 100), batch (24 / 70) loss = (G: 6.58258915, D: 0.04448618) (3.520 sec/batch) (ETA: 25144 seconds)\n",
            "cycle (0 / 100), batch (25 / 70) loss = (G: 7.04501724, D: 0.06506850) (3.570 sec/batch) (ETA: 25148 seconds)\n",
            "cycle (0 / 100), batch (26 / 70) loss = (G: 5.68111658, D: 0.21430057) (3.501 sec/batch) (ETA: 25094 seconds)\n",
            "cycle (0 / 100), batch (27 / 70) loss = (G: 3.01159525, D: 0.06145286) (3.529 sec/batch) (ETA: 25083 seconds)\n",
            "cycle (0 / 100), batch (28 / 70) loss = (G: 3.82921267, D: 0.10399988) (3.538 sec/batch) (ETA: 25068 seconds)\n",
            "cycle (0 / 100), batch (29 / 70) loss = (G: 4.62823248, D: 0.03752480) (3.517 sec/batch) (ETA: 25039 seconds)\n",
            "cycle (0 / 100), batch (30 / 70) loss = (G: 7.15359163, D: 0.02753101) (3.459 sec/batch) (ETA: 24986 seconds)\n",
            "cycle (0 / 100), batch (31 / 70) loss = (G: 2.89635777, D: 1.28717446) (3.486 sec/batch) (ETA: 24970 seconds)\n",
            "cycle (0 / 100), batch (32 / 70) loss = (G: 11.33376503, D: 1.20076418) (3.505 sec/batch) (ETA: 24956 seconds)\n",
            "cycle (0 / 100), batch (33 / 70) loss = (G: 9.71275139, D: 0.48979878) (3.542 sec/batch) (ETA: 24956 seconds)\n",
            "cycle (0 / 100), batch (34 / 70) loss = (G: 8.01921463, D: 0.25371388) (3.533 sec/batch) (ETA: 24940 seconds)\n",
            "cycle (0 / 100), batch (35 / 70) loss = (G: 6.24044466, D: 0.21825813) (3.489 sec/batch) (ETA: 24904 seconds)\n",
            "cycle (0 / 100), batch (36 / 70) loss = (G: 4.12026167, D: 0.03678431) (3.496 sec/batch) (ETA: 24887 seconds)\n",
            "cycle (0 / 100), batch (37 / 70) loss = (G: 2.21150994, D: 0.36088184) (3.527 sec/batch) (ETA: 24885 seconds)\n",
            "cycle (0 / 100), batch (38 / 70) loss = (G: 3.28578091, D: 0.05339941) (3.517 sec/batch) (ETA: 24868 seconds)\n",
            "cycle (0 / 100), batch (39 / 70) loss = (G: 3.62003589, D: 0.04232367) (3.477 sec/batch) (ETA: 24835 seconds)\n",
            "cycle (0 / 100), batch (40 / 70) loss = (G: 5.77980614, D: 0.03564982) (3.522 sec/batch) (ETA: 24838 seconds)\n",
            "cycle (0 / 100), batch (41 / 70) loss = (G: 3.15532017, D: 0.14425729) (3.498 sec/batch) (ETA: 24815 seconds)\n",
            "cycle (0 / 100), batch (42 / 70) loss = (G: 12.28360176, D: 0.24103041) (3.474 sec/batch) (ETA: 24788 seconds)\n",
            "cycle (0 / 100), batch (43 / 70) loss = (G: 11.21118832, D: 4.64922333) (3.476 sec/batch) (ETA: 24772 seconds)\n",
            "cycle (0 / 100), batch (44 / 70) loss = (G: 9.47258091, D: 3.60167742) (3.551 sec/batch) (ETA: 24792 seconds)\n",
            "cycle (0 / 100), batch (45 / 70) loss = (G: 7.82433128, D: 0.08515058) (3.554 sec/batch) (ETA: 24787 seconds)\n",
            "cycle (0 / 100), batch (46 / 70) loss = (G: 5.28202486, D: 0.13738653) (3.520 sec/batch) (ETA: 24766 seconds)\n",
            "cycle (0 / 100), batch (47 / 70) loss = (G: 2.35349560, D: 0.14529821) (3.518 sec/batch) (ETA: 24755 seconds)\n",
            "cycle (0 / 100), batch (48 / 70) loss = (G: 2.64518571, D: 0.11071872) (3.459 sec/batch) (ETA: 24717 seconds)\n",
            "cycle (0 / 100), batch (49 / 70) loss = (G: 4.04172802, D: 0.05640681) (3.482 sec/batch) (ETA: 24711 seconds)\n",
            "cycle (0 / 100), batch (50 / 70) loss = (G: 4.02634954, D: 0.53040493) (3.460 sec/batch) (ETA: 24687 seconds)\n",
            "cycle (0 / 100), batch (51 / 70) loss = (G: 7.67988205, D: 0.08282591) (3.615 sec/batch) (ETA: 24745 seconds)\n",
            "cycle (0 / 100), batch (52 / 70) loss = (G: 7.32293510, D: 0.04806967) (3.574 sec/batch) (ETA: 24729 seconds)\n",
            "cycle (0 / 100), batch (53 / 70) loss = (G: 5.50113583, D: 0.68163002) (3.604 sec/batch) (ETA: 24741 seconds)\n",
            "cycle (0 / 100), batch (54 / 70) loss = (G: 1.05344844, D: 0.48071572) (3.540 sec/batch) (ETA: 24713 seconds)\n",
            "cycle (0 / 100), batch (55 / 70) loss = (G: 8.89493179, D: 0.03405687) (3.494 sec/batch) (ETA: 24686 seconds)\n",
            "cycle (0 / 100), batch (56 / 70) loss = (G: 8.93133354, D: 0.01635563) (3.471 sec/batch) (ETA: 24664 seconds)\n",
            "cycle (0 / 100), batch (57 / 70) loss = (G: 8.04537296, D: 0.02314980) (3.468 sec/batch) (ETA: 24649 seconds)\n",
            "cycle (0 / 100), batch (58 / 70) loss = (G: 7.19369888, D: 0.04107906) (3.458 sec/batch) (ETA: 24632 seconds)\n",
            "cycle (0 / 100), batch (59 / 70) loss = (G: 6.07700729, D: 0.00969506) (3.533 sec/batch) (ETA: 24652 seconds)\n",
            "cycle (0 / 100), batch (60 / 70) loss = (G: 4.83935452, D: 0.02136046) (3.576 sec/batch) (ETA: 24666 seconds)\n",
            "cycle (0 / 100), batch (61 / 70) loss = (G: 3.26578426, D: 0.05034797) (3.481 sec/batch) (ETA: 24621 seconds)\n",
            "cycle (0 / 100), batch (62 / 70) loss = (G: 1.99847984, D: 0.15159714) (3.520 sec/batch) (ETA: 24628 seconds)\n",
            "cycle (0 / 100), batch (63 / 70) loss = (G: 5.17106962, D: 0.02308761) (3.462 sec/batch) (ETA: 24595 seconds)\n",
            "cycle (0 / 100), batch (64 / 70) loss = (G: 4.78874445, D: 0.66278768) (3.496 sec/batch) (ETA: 24598 seconds)\n",
            "cycle (0 / 100), batch (65 / 70) loss = (G: 2.66039753, D: 0.09838872) (3.452 sec/batch) (ETA: 24569 seconds)\n",
            "cycle (0 / 100), batch (66 / 70) loss = (G: 9.94719315, D: 0.03832347) (3.738 sec/batch) (ETA: 24684 seconds)\n",
            "cycle (0 / 100), batch (67 / 70) loss = (G: 9.69156075, D: 0.06191235) (3.634 sec/batch) (ETA: 24652 seconds)\n",
            "cycle (0 / 100), batch (68 / 70) loss = (G: 8.47093391, D: 0.13443473) (3.708 sec/batch) (ETA: 24689 seconds)\n",
            "cycle (0 / 100), batch (69 / 70) loss = (G: 6.97200584, D: 0.07185160) (3.576 sec/batch) (ETA: 24641 seconds)\n",
            "train.py:119: DeprecationWarning:     `imsave` is deprecated!\n",
            "    `imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imwrite`` instead.\n",
            "  img_channel.reshape(128, 128))\n",
            "train.py:121: DeprecationWarning:     `imsave` is deprecated!\n",
            "    `imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imwrite`` instead.\n",
            "  img_seg.reshape(128, 128))\n",
            "train.py:136: DeprecationWarning:     `imsave` is deprecated!\n",
            "    `imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imwrite`` instead.\n",
            "  img_channel.reshape(128, 128))\n",
            "train.py:138: DeprecationWarning:     `imsave` is deprecated!\n",
            "    `imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imwrite`` instead.\n",
            "  img_seg.reshape(128, 128))\n",
            "cycle (1 / 100), batch (0 / 70) loss = (G: 5.49238729, D: 0.07027911) (3.640 sec/batch) (ETA: 24668 seconds)\n",
            "cycle (1 / 100), batch (1 / 70) loss = (G: 3.49816418, D: 0.05958343) (3.749 sec/batch) (ETA: 24720 seconds)\n",
            "cycle (1 / 100), batch (2 / 70) loss = (G: 1.99940681, D: 0.17778608) (3.672 sec/batch) (ETA: 24700 seconds)\n",
            "cycle (1 / 100), batch (3 / 70) loss = (G: 5.63834524, D: 0.12137187) (3.689 sec/batch) (ETA: 24714 seconds)\n",
            "cycle (1 / 100), batch (4 / 70) loss = (G: 5.40364265, D: 0.02195783) (3.653 sec/batch) (ETA: 24706 seconds)\n",
            "cycle (1 / 100), batch (5 / 70) loss = (G: 3.48579478, D: 0.04890076) (3.633 sec/batch) (ETA: 24701 seconds)\n",
            "cycle (1 / 100), batch (6 / 70) loss = (G: 6.38663101, D: 0.13624828) (3.682 sec/batch) (ETA: 24725 seconds)\n",
            "cycle (1 / 100), batch (7 / 70) loss = (G: 3.28368592, D: 0.34331974) (3.479 sec/batch) (ETA: 24644 seconds)\n",
            "cycle (1 / 100), batch (8 / 70) loss = (G: 14.28862858, D: 0.64313161) (3.558 sec/batch) (ETA: 24667 seconds)\n",
            "cycle (1 / 100), batch (9 / 70) loss = (G: 12.91693783, D: 0.30098262) (3.496 sec/batch) (ETA: 24636 seconds)\n",
            "cycle (1 / 100), batch (10 / 70) loss = (G: 11.51150799, D: 0.21350119) (3.480 sec/batch) (ETA: 24620 seconds)\n",
            "cycle (1 / 100), batch (11 / 70) loss = (G: 10.05138779, D: 0.04275764) (3.548 sec/batch) (ETA: 24639 seconds)\n",
            "cycle (1 / 100), batch (12 / 70) loss = (G: 8.46175575, D: 0.04085654) (3.508 sec/batch) (ETA: 24617 seconds)\n",
            "cycle (1 / 100), batch (13 / 70) loss = (G: 6.92947435, D: 0.01162133) (3.536 sec/batch) (ETA: 24622 seconds)\n",
            "cycle (1 / 100), batch (14 / 70) loss = (G: 5.04488707, D: 0.07995438) (3.532 sec/batch) (ETA: 24614 seconds)\n",
            "cycle (1 / 100), batch (15 / 70) loss = (G: 3.07608247, D: 0.06483225) (3.566 sec/batch) (ETA: 24623 seconds)\n",
            "cycle (1 / 100), batch (16 / 70) loss = (G: 2.12737989, D: 0.13551041) (3.501 sec/batch) (ETA: 24592 seconds)\n",
            "cycle (1 / 100), batch (17 / 70) loss = (G: 6.32620525, D: 0.30460426) (3.468 sec/batch) (ETA: 24570 seconds)\n",
            "cycle (1 / 100), batch (18 / 70) loss = (G: 6.63247013, D: 0.01159413) (3.535 sec/batch) (ETA: 24588 seconds)\n",
            "cycle (1 / 100), batch (19 / 70) loss = (G: 5.81917095, D: 1.05516350) (3.520 sec/batch) (ETA: 24576 seconds)\n",
            "cycle (1 / 100), batch (20 / 70) loss = (G: 3.66670918, D: 0.03314205) (3.485 sec/batch) (ETA: 24556 seconds)\n",
            "cycle (1 / 100), batch (21 / 70) loss = (G: 2.57353210, D: 0.08277954) (3.440 sec/batch) (ETA: 24528 seconds)\n",
            "cycle (1 / 100), batch (22 / 70) loss = (G: 7.80478382, D: 0.14952159) (3.478 sec/batch) (ETA: 24532 seconds)\n",
            "cycle (1 / 100), batch (23 / 70) loss = (G: 7.72252798, D: 0.04219151) (3.402 sec/batch) (ETA: 24491 seconds)\n",
            "cycle (1 / 100), batch (24 / 70) loss = (G: 6.74140072, D: 2.43632603) (3.482 sec/batch) (ETA: 24511 seconds)\n",
            "cycle (1 / 100), batch (25 / 70) loss = (G: 4.53914213, D: 0.31581861) (3.492 sec/batch) (ETA: 24506 seconds)\n",
            "cycle (1 / 100), batch (26 / 70) loss = (G: 1.82860136, D: 0.20106253) (3.507 sec/batch) (ETA: 24505 seconds)\n",
            "cycle (1 / 100), batch (27 / 70) loss = (G: 7.21377373, D: 0.03166332) (3.493 sec/batch) (ETA: 24492 seconds)\n",
            "cycle (1 / 100), batch (28 / 70) loss = (G: 7.14317846, D: 0.63555509) (3.484 sec/batch) (ETA: 24482 seconds)\n",
            "cycle (1 / 100), batch (29 / 70) loss = (G: 4.14805317, D: 0.02661525) (3.563 sec/batch) (ETA: 24506 seconds)\n",
            "cycle (1 / 100), batch (30 / 70) loss = (G: 1.45939660, D: 0.27048615) (3.497 sec/batch) (ETA: 24476 seconds)\n",
            "cycle (1 / 100), batch (31 / 70) loss = (G: 7.94371176, D: 0.07026223) (3.516 sec/batch) (ETA: 24477 seconds)\n",
            "cycle (1 / 100), batch (32 / 70) loss = (G: 8.60822678, D: 0.14382418) (3.489 sec/batch) (ETA: 24460 seconds)\n",
            "cycle (1 / 100), batch (33 / 70) loss = (G: 8.26524162, D: 0.04086687) (3.513 sec/batch) (ETA: 24462 seconds)\n",
            "cycle (1 / 100), batch (34 / 70) loss = (G: 7.72657156, D: 0.17656459) (3.510 sec/batch) (ETA: 24455 seconds)\n",
            "cycle (1 / 100), batch (35 / 70) loss = (G: 6.59191990, D: 0.95366490) (3.462 sec/batch) (ETA: 24430 seconds)\n",
            "cycle (1 / 100), batch (36 / 70) loss = (G: 5.25110388, D: 0.04980553) (3.532 sec/batch) (ETA: 24450 seconds)\n",
            "cycle (1 / 100), batch (37 / 70) loss = (G: 3.69004679, D: 0.07196030) (3.521 sec/batch) (ETA: 24441 seconds)\n",
            "cycle (1 / 100), batch (38 / 70) loss = (G: 2.77357125, D: 0.11106770) (3.516 sec/batch) (ETA: 24434 seconds)\n",
            "cycle (1 / 100), batch (39 / 70) loss = (G: 4.25188684, D: 0.03090616) (3.486 sec/batch) (ETA: 24416 seconds)\n",
            "cycle (1 / 100), batch (40 / 70) loss = (G: 4.77173615, D: 0.33021900) (3.500 sec/batch) (ETA: 24414 seconds)\n",
            "cycle (1 / 100), batch (41 / 70) loss = (G: 2.74054098, D: 0.08887688) (3.456 sec/batch) (ETA: 24390 seconds)\n",
            "cycle (1 / 100), batch (42 / 70) loss = (G: 5.74656010, D: 0.27249551) (3.477 sec/batch) (ETA: 24390 seconds)\n",
            "cycle (1 / 100), batch (43 / 70) loss = (G: 3.68637705, D: 0.14253221) (3.434 sec/batch) (ETA: 24365 seconds)\n",
            "cycle (1 / 100), batch (44 / 70) loss = (G: 3.41905928, D: 0.36258045) (3.526 sec/batch) (ETA: 24393 seconds)\n",
            "cycle (1 / 100), batch (45 / 70) loss = (G: 7.37214088, D: 1.21167755) (3.473 sec/batch) (ETA: 24367 seconds)\n",
            "cycle (1 / 100), batch (46 / 70) loss = (G: 4.66789055, D: 0.32865593) (3.472 sec/batch) (ETA: 24359 seconds)\n",
            "cycle (1 / 100), batch (47 / 70) loss = (G: 1.49576628, D: 0.45030385) (3.438 sec/batch) (ETA: 24338 seconds)\n",
            "cycle (1 / 100), batch (48 / 70) loss = (G: 6.83755302, D: 0.44025397) (3.480 sec/batch) (ETA: 24346 seconds)\n",
            "cycle (1 / 100), batch (49 / 70) loss = (G: 0.59386516, D: 1.45925939) (3.451 sec/batch) (ETA: 24327 seconds)\n",
            "cycle (1 / 100), batch (50 / 70) loss = (G: 9.83992481, D: 0.37450257) (3.474 sec/batch) (ETA: 24328 seconds)\n",
            "cycle (1 / 100), batch (51 / 70) loss = (G: 3.33646393, D: 0.22317743) (3.433 sec/batch) (ETA: 24305 seconds)\n",
            "cycle (1 / 100), batch (52 / 70) loss = (G: 5.51170158, D: 0.10667683) (3.409 sec/batch) (ETA: 24286 seconds)\n",
            "cycle (1 / 100), batch (53 / 70) loss = (G: 2.45718074, D: 13.11872959) (3.444 sec/batch) (ETA: 24289 seconds)\n",
            "cycle (1 / 100), batch (54 / 70) loss = (G: 1.08602750, D: 0.63902617) (3.436 sec/batch) (ETA: 24278 seconds)\n",
            "cycle (1 / 100), batch (55 / 70) loss = (G: 10.12993813, D: 1.00506938) (3.428 sec/batch) (ETA: 24266 seconds)\n",
            "cycle (1 / 100), batch (56 / 70) loss = (G: 5.36325264, D: 0.96972388) (3.421 sec/batch) (ETA: 24254 seconds)\n",
            "cycle (1 / 100), batch (57 / 70) loss = (G: 1.79183304, D: 1.41709411) (3.457 sec/batch) (ETA: 24259 seconds)\n",
            "cycle (1 / 100), batch (58 / 70) loss = (G: 2.26646447, D: 0.90873313) (3.392 sec/batch) (ETA: 24226 seconds)\n",
            "cycle (1 / 100), batch (59 / 70) loss = (G: 6.84848404, D: 0.03636198) (3.404 sec/batch) (ETA: 24220 seconds)\n",
            "cycle (1 / 100), batch (60 / 70) loss = (G: 9.33458328, D: 0.42396364) (3.454 sec/batch) (ETA: 24230 seconds)\n",
            "cycle (1 / 100), batch (61 / 70) loss = (G: 6.83740234, D: 0.09630863) (3.464 sec/batch) (ETA: 24227 seconds)\n",
            "cycle (1 / 100), batch (62 / 70) loss = (G: 1.52603459, D: 0.35859767) (3.425 sec/batch) (ETA: 24204 seconds)\n",
            "cycle (1 / 100), batch (63 / 70) loss = (G: 1.06262481, D: 0.58210945) (3.414 sec/batch) (ETA: 24191 seconds)\n",
            "cycle (1 / 100), batch (64 / 70) loss = (G: 5.18676519, D: 0.72859079) (3.414 sec/batch) (ETA: 24182 seconds)\n",
            "cycle (1 / 100), batch (65 / 70) loss = (G: 6.52220774, D: 0.07049306) (3.419 sec/batch) (ETA: 24175 seconds)\n",
            "cycle (1 / 100), batch (66 / 70) loss = (G: 6.41816807, D: 0.18107969) (3.426 sec/batch) (ETA: 24169 seconds)\n",
            "cycle (1 / 100), batch (67 / 70) loss = (G: 3.34549689, D: 0.35241878) (3.463 sec/batch) (ETA: 24175 seconds)\n",
            "cycle (1 / 100), batch (68 / 70) loss = (G: 2.57362771, D: 0.16373564) (3.329 sec/batch) (ETA: 24117 seconds)\n",
            "cycle (1 / 100), batch (69 / 70) loss = (G: 4.88634872, D: 0.25017050) (3.393 sec/batch) (ETA: 24129 seconds)\n",
            "cycle (2 / 100), batch (0 / 70) loss = (G: 3.01261997, D: 0.06014746) (3.424 sec/batch) (ETA: 24131 seconds)\n",
            "cycle (2 / 100), batch (1 / 70) loss = (G: 12.40766430, D: 0.49407315) (3.442 sec/batch) (ETA: 24130 seconds)\n",
            "cycle (2 / 100), batch (2 / 70) loss = (G: 11.78804588, D: 1.91958308) (3.376 sec/batch) (ETA: 24098 seconds)\n",
            "cycle (2 / 100), batch (3 / 70) loss = (G: 9.54476452, D: 3.87576485) (3.393 sec/batch) (ETA: 24094 seconds)\n",
            "cycle (2 / 100), batch (4 / 70) loss = (G: 0.39881206, D: 1.65327406) (3.426 sec/batch) (ETA: 24098 seconds)\n",
            "cycle (2 / 100), batch (5 / 70) loss = (G: 0.05996886, D: 3.63601899) (3.414 sec/batch) (ETA: 24085 seconds)\n",
            "cycle (2 / 100), batch (6 / 70) loss = (G: 1.88266146, D: 0.70464981) (3.357 sec/batch) (ETA: 24055 seconds)\n",
            "cycle (2 / 100), batch (7 / 70) loss = (G: 5.31876278, D: 0.80438942) (3.422 sec/batch) (ETA: 24069 seconds)\n",
            "cycle (2 / 100), batch (8 / 70) loss = (G: 3.80852890, D: 0.06071456) (3.427 sec/batch) (ETA: 24064 seconds)\n",
            "cycle (2 / 100), batch (9 / 70) loss = (G: 3.29255009, D: 0.13941993) (3.406 sec/batch) (ETA: 24048 seconds)\n",
            "cycle (2 / 100), batch (10 / 70) loss = (G: 2.08128810, D: 0.25134519) (3.391 sec/batch) (ETA: 24034 seconds)\n",
            "cycle (2 / 100), batch (11 / 70) loss = (G: 5.38320589, D: 0.13723323) (3.456 sec/batch) (ETA: 24050 seconds)\n",
            "cycle (2 / 100), batch (12 / 70) loss = (G: 3.26269102, D: 0.21239440) (3.414 sec/batch) (ETA: 24028 seconds)\n",
            "cycle (2 / 100), batch (13 / 70) loss = (G: 4.01556206, D: 0.78843939) (3.457 sec/batch) (ETA: 24037 seconds)\n",
            "cycle (2 / 100), batch (14 / 70) loss = (G: 4.88827419, D: 0.65897834) (3.434 sec/batch) (ETA: 24022 seconds)\n",
            "cycle (2 / 100), batch (15 / 70) loss = (G: 7.24050522, D: 1.16672671) (3.428 sec/batch) (ETA: 24013 seconds)\n",
            "cycle (2 / 100), batch (16 / 70) loss = (G: 3.54967713, D: 0.17245091) (3.472 sec/batch) (ETA: 24023 seconds)\n",
            "cycle (2 / 100), batch (17 / 70) loss = (G: 13.49275398, D: 0.03742599) (3.397 sec/batch) (ETA: 23989 seconds)\n",
            "cycle (2 / 100), batch (18 / 70) loss = (G: 0.80497658, D: 0.74670601) (3.397 sec/batch) (ETA: 23981 seconds)\n",
            "cycle (2 / 100), batch (19 / 70) loss = (G: 2.85919976, D: 0.11266866) (3.384 sec/batch) (ETA: 23968 seconds)\n",
            "cycle (2 / 100), batch (20 / 70) loss = (G: 5.04334784, D: 0.19547863) (3.371 sec/batch) (ETA: 23954 seconds)\n",
            "cycle (2 / 100), batch (21 / 70) loss = (G: 4.01919651, D: 0.35741827) (3.397 sec/batch) (ETA: 23955 seconds)\n",
            "cycle (2 / 100), batch (22 / 70) loss = (G: 1.46579707, D: 0.82537740) (3.395 sec/batch) (ETA: 23946 seconds)\n",
            "cycle (2 / 100), batch (23 / 70) loss = (G: 1.94520211, D: 0.21081042) (3.367 sec/batch) (ETA: 23928 seconds)\n",
            "cycle (2 / 100), batch (24 / 70) loss = (G: 3.96314430, D: 0.30566308) (3.465 sec/batch) (ETA: 23956 seconds)\n",
            "cycle (2 / 100), batch (25 / 70) loss = (G: 3.54132795, D: 0.22434944) (3.406 sec/batch) (ETA: 23928 seconds)\n",
            "cycle (2 / 100), batch (26 / 70) loss = (G: 2.52418137, D: 0.48091149) (3.463 sec/batch) (ETA: 23943 seconds)\n",
            "cycle (2 / 100), batch (27 / 70) loss = (G: 2.27642727, D: 0.12974475) (3.518 sec/batch) (ETA: 23958 seconds)\n",
            "cycle (2 / 100), batch (28 / 70) loss = (G: 3.12812185, D: 0.07674468) (3.608 sec/batch) (ETA: 23989 seconds)\n",
            "cycle (2 / 100), batch (29 / 70) loss = (G: 3.14143777, D: 0.09298845) (3.578 sec/batch) (ETA: 23979 seconds)\n",
            "cycle (2 / 100), batch (30 / 70) loss = (G: 5.13988876, D: 0.25060359) (3.531 sec/batch) (ETA: 23960 seconds)\n",
            "cycle (2 / 100), batch (31 / 70) loss = (G: 3.46854877, D: 0.08235529) (3.382 sec/batch) (ETA: 23901 seconds)\n",
            "cycle (2 / 100), batch (32 / 70) loss = (G: 3.49974489, D: 0.19851141) (3.429 sec/batch) (ETA: 23910 seconds)\n",
            "cycle (2 / 100), batch (33 / 70) loss = (G: 2.97581506, D: 0.11349759) (3.406 sec/batch) (ETA: 23895 seconds)\n",
            "cycle (2 / 100), batch (34 / 70) loss = (G: 6.76315069, D: 0.01954093) (3.400 sec/batch) (ETA: 23886 seconds)\n",
            "cycle (2 / 100), batch (35 / 70) loss = (G: 4.47535467, D: 0.13123225) (3.459 sec/batch) (ETA: 23901 seconds)\n",
            "cycle (2 / 100), batch (36 / 70) loss = (G: 4.11632347, D: 0.11271370) (3.487 sec/batch) (ETA: 23906 seconds)\n",
            "cycle (2 / 100), batch (37 / 70) loss = (G: 3.55889988, D: 0.13360162) (3.476 sec/batch) (ETA: 23898 seconds)\n",
            "cycle (2 / 100), batch (38 / 70) loss = (G: 3.77540779, D: 0.11284378) (3.464 sec/batch) (ETA: 23889 seconds)\n",
            "cycle (2 / 100), batch (39 / 70) loss = (G: 3.53563261, D: 1.39129257) (3.498 sec/batch) (ETA: 23897 seconds)\n",
            "cycle (2 / 100), batch (40 / 70) loss = (G: 1.09451008, D: 0.43905365) (3.422 sec/batch) (ETA: 23864 seconds)\n",
            "cycle (2 / 100), batch (41 / 70) loss = (G: 4.05165529, D: 0.12836382) (3.390 sec/batch) (ETA: 23846 seconds)\n",
            "cycle (2 / 100), batch (42 / 70) loss = (G: 4.52771950, D: 0.10236693) (3.435 sec/batch) (ETA: 23855 seconds)\n",
            "cycle (2 / 100), batch (43 / 70) loss = (G: 3.74465513, D: 0.08515699) (3.464 sec/batch) (ETA: 23860 seconds)\n",
            "cycle (2 / 100), batch (44 / 70) loss = (G: 3.97822523, D: 0.10215407) (3.413 sec/batch) (ETA: 23836 seconds)\n",
            "cycle (2 / 100), batch (45 / 70) loss = (G: 2.82874584, D: 1.04425001) (3.436 sec/batch) (ETA: 23838 seconds)\n",
            "cycle (2 / 100), batch (46 / 70) loss = (G: 3.80834818, D: 0.49585137) (3.433 sec/batch) (ETA: 23831 seconds)\n",
            "cycle (2 / 100), batch (47 / 70) loss = (G: 2.12332058, D: 0.15762748) (3.398 sec/batch) (ETA: 23813 seconds)\n",
            "cycle (2 / 100), batch (48 / 70) loss = (G: 2.64920616, D: 0.15707640) (3.436 sec/batch) (ETA: 23820 seconds)\n",
            "cycle (2 / 100), batch (49 / 70) loss = (G: 3.74743938, D: 0.69382668) (3.402 sec/batch) (ETA: 23801 seconds)\n",
            "cycle (2 / 100), batch (50 / 70) loss = (G: 2.59880209, D: 0.27689829) (3.406 sec/batch) (ETA: 23796 seconds)\n",
            "cycle (2 / 100), batch (51 / 70) loss = (G: 1.79836488, D: 0.79081583) (3.462 sec/batch) (ETA: 23810 seconds)\n",
            "cycle (2 / 100), batch (52 / 70) loss = (G: 3.20990348, D: 0.10507606) (3.417 sec/batch) (ETA: 23789 seconds)\n",
            "cycle (2 / 100), batch (53 / 70) loss = (G: 8.22178650, D: 2.60095835) (3.419 sec/batch) (ETA: 23783 seconds)\n",
            "cycle (2 / 100), batch (54 / 70) loss = (G: 6.97653389, D: 5.73888063) (3.439 sec/batch) (ETA: 23784 seconds)\n",
            "cycle (2 / 100), batch (55 / 70) loss = (G: 4.49743271, D: 0.08418921) (3.356 sec/batch) (ETA: 23748 seconds)\n",
            "cycle (2 / 100), batch (56 / 70) loss = (G: 2.69082665, D: 0.23131739) (3.388 sec/batch) (ETA: 23752 seconds)\n",
            "cycle (2 / 100), batch (57 / 70) loss = (G: 5.15576649, D: 0.02181651) (3.449 sec/batch) (ETA: 23768 seconds)\n",
            "cycle (2 / 100), batch (58 / 70) loss = (G: 1.65322518, D: 0.25793052) (3.433 sec/batch) (ETA: 23756 seconds)\n",
            "cycle (2 / 100), batch (59 / 70) loss = (G: 8.47239876, D: 0.17193432) (3.391 sec/batch) (ETA: 23735 seconds)\n",
            "cycle (2 / 100), batch (60 / 70) loss = (G: 9.24964714, D: 0.69510585) (3.405 sec/batch) (ETA: 23734 seconds)\n",
            "cycle (2 / 100), batch (61 / 70) loss = (G: 4.97106123, D: 0.54750210) (3.465 sec/batch) (ETA: 23750 seconds)\n",
            "cycle (2 / 100), batch (62 / 70) loss = (G: 4.04300404, D: 0.36182776) (3.431 sec/batch) (ETA: 23732 seconds)\n",
            "cycle (2 / 100), batch (63 / 70) loss = (G: 3.58280659, D: 1.87654328) (3.433 sec/batch) (ETA: 23728 seconds)\n",
            "cycle (2 / 100), batch (64 / 70) loss = (G: 2.82804680, D: 0.18743464) (3.435 sec/batch) (ETA: 23723 seconds)\n",
            "cycle (2 / 100), batch (65 / 70) loss = (G: 2.38016272, D: 0.11216655) (3.421 sec/batch) (ETA: 23712 seconds)\n",
            "cycle (2 / 100), batch (66 / 70) loss = (G: 3.10766840, D: 0.05815060) (3.407 sec/batch) (ETA: 23702 seconds)\n",
            "cycle (2 / 100), batch (67 / 70) loss = (G: 2.71328044, D: 0.08615568) (3.408 sec/batch) (ETA: 23696 seconds)\n",
            "cycle (2 / 100), batch (68 / 70) loss = (G: 3.71806145, D: 0.05833107) (3.408 sec/batch) (ETA: 23690 seconds)\n",
            "cycle (2 / 100), batch (69 / 70) loss = (G: 5.54118061, D: 0.01310723) (3.400 sec/batch) (ETA: 23680 seconds)\n",
            "cycle (3 / 100), batch (0 / 70) loss = (G: 4.11387968, D: 0.09133095) (3.391 sec/batch) (ETA: 23671 seconds)\n",
            "cycle (3 / 100), batch (1 / 70) loss = (G: 1.53929234, D: 0.25883827) (3.433 sec/batch) (ETA: 23680 seconds)\n",
            "cycle (3 / 100), batch (2 / 70) loss = (G: 7.65680742, D: 0.40167329) (3.477 sec/batch) (ETA: 23691 seconds)\n",
            "cycle (3 / 100), batch (3 / 70) loss = (G: 8.54228878, D: 5.83469296) (3.434 sec/batch) (ETA: 23671 seconds)\n",
            "cycle (3 / 100), batch (4 / 70) loss = (G: 9.77500725, D: 6.38023472) (3.421 sec/batch) (ETA: 23661 seconds)\n",
            "cycle (3 / 100), batch (5 / 70) loss = (G: 4.46211958, D: 0.23092113) (3.431 sec/batch) (ETA: 23659 seconds)\n",
            "cycle (3 / 100), batch (6 / 70) loss = (G: 1.88722003, D: 0.58995861) (3.446 sec/batch) (ETA: 23660 seconds)\n",
            "cycle (3 / 100), batch (7 / 70) loss = (G: 1.22283208, D: 0.37521285) (3.401 sec/batch) (ETA: 23638 seconds)\n",
            "cycle (3 / 100), batch (8 / 70) loss = (G: 3.02735877, D: 0.08539321) (3.426 sec/batch) (ETA: 23641 seconds)\n",
            "cycle (3 / 100), batch (9 / 70) loss = (G: 4.12290764, D: 0.06398398) (3.398 sec/batch) (ETA: 23626 seconds)\n",
            "cycle (3 / 100), batch (10 / 70) loss = (G: 3.97327065, D: 0.07143492) (3.389 sec/batch) (ETA: 23616 seconds)\n",
            "cycle (3 / 100), batch (11 / 70) loss = (G: 2.78609180, D: 0.09917878) (3.452 sec/batch) (ETA: 23633 seconds)\n",
            "cycle (3 / 100), batch (12 / 70) loss = (G: 3.69183135, D: 0.06878452) (3.411 sec/batch) (ETA: 23613 seconds)\n",
            "cycle (3 / 100), batch (13 / 70) loss = (G: 3.03856826, D: 0.09198792) (3.463 sec/batch) (ETA: 23627 seconds)\n",
            "cycle (3 / 100), batch (14 / 70) loss = (G: 3.45739961, D: 0.04831953) (3.456 sec/batch) (ETA: 23620 seconds)\n",
            "cycle (3 / 100), batch (15 / 70) loss = (G: 2.30454302, D: 0.21100430) (3.457 sec/batch) (ETA: 23616 seconds)\n",
            "cycle (3 / 100), batch (16 / 70) loss = (G: 4.16325855, D: 0.03906351) (3.447 sec/batch) (ETA: 23608 seconds)\n",
            "cycle (3 / 100), batch (17 / 70) loss = (G: 4.12733126, D: 0.03551062) (3.413 sec/batch) (ETA: 23591 seconds)\n",
            "cycle (3 / 100), batch (18 / 70) loss = (G: 4.83527994, D: 0.02341334) (3.436 sec/batch) (ETA: 23594 seconds)\n",
            "cycle (3 / 100), batch (19 / 70) loss = (G: 4.19399929, D: 0.13337930) (3.452 sec/batch) (ETA: 23595 seconds)\n",
            "cycle (3 / 100), batch (20 / 70) loss = (G: 4.28649330, D: 0.08679292) (3.455 sec/batch) (ETA: 23591 seconds)\n",
            "cycle (3 / 100), batch (21 / 70) loss = (G: 1.69765234, D: 0.66826069) (3.520 sec/batch) (ETA: 23611 seconds)\n",
            "cycle (3 / 100), batch (22 / 70) loss = (G: 9.07693481, D: 3.44449139) (3.533 sec/batch) (ETA: 23613 seconds)\n",
            "cycle (3 / 100), batch (23 / 70) loss = (G: 9.83877659, D: 0.13074340) (3.442 sec/batch) (ETA: 23577 seconds)\n",
            "cycle (3 / 100), batch (24 / 70) loss = (G: 5.89058924, D: 0.35827833) (3.461 sec/batch) (ETA: 23580 seconds)\n",
            "cycle (3 / 100), batch (25 / 70) loss = (G: 5.41875362, D: 0.24240276) (3.441 sec/batch) (ETA: 23568 seconds)\n",
            "cycle (3 / 100), batch (26 / 70) loss = (G: 3.33348346, D: 0.20837325) (3.481 sec/batch) (ETA: 23578 seconds)\n",
            "cycle (3 / 100), batch (27 / 70) loss = (G: 4.17808247, D: 0.04424817) (3.444 sec/batch) (ETA: 23561 seconds)\n",
            "cycle (3 / 100), batch (28 / 70) loss = (G: 2.94223833, D: 0.13224384) (3.495 sec/batch) (ETA: 23575 seconds)\n",
            "cycle (3 / 100), batch (29 / 70) loss = (G: 3.52594519, D: 0.29981679) (3.491 sec/batch) (ETA: 23570 seconds)\n",
            "cycle (3 / 100), batch (30 / 70) loss = (G: 1.45096445, D: 1.09217501) (3.552 sec/batch) (ETA: 23589 seconds)\n",
            "cycle (3 / 100), batch (31 / 70) loss = (G: 3.33901119, D: 0.08445936) (3.518 sec/batch) (ETA: 23575 seconds)\n",
            "cycle (3 / 100), batch (32 / 70) loss = (G: 4.09356689, D: 0.05335030) (3.547 sec/batch) (ETA: 23583 seconds)\n",
            "cycle (3 / 100), batch (33 / 70) loss = (G: 5.06242371, D: 0.22537985) (3.743 sec/batch) (ETA: 23652 seconds)\n",
            "cycle (3 / 100), batch (34 / 70) loss = (G: 3.96806812, D: 0.22137563) (3.732 sec/batch) (ETA: 23651 seconds)\n",
            "cycle (3 / 100), batch (35 / 70) loss = (G: 3.17718506, D: 0.05140187) (3.541 sec/batch) (ETA: 23585 seconds)\n",
            "cycle (3 / 100), batch (36 / 70) loss = (G: 5.08934212, D: 0.44803110) (3.608 sec/batch) (ETA: 23607 seconds)\n",
            "cycle (3 / 100), batch (37 / 70) loss = (G: 5.19837952, D: 0.03080825) (3.770 sec/batch) (ETA: 23665 seconds)\n",
            "cycle (3 / 100), batch (38 / 70) loss = (G: 4.40144920, D: 0.33665583) (3.806 sec/batch) (ETA: 23682 seconds)\n",
            "cycle (3 / 100), batch (39 / 70) loss = (G: 1.87059283, D: 0.18836489) (3.836 sec/batch) (ETA: 23698 seconds)\n",
            "cycle (3 / 100), batch (40 / 70) loss = (G: 0.75163364, D: 0.66337609) (3.817 sec/batch) (ETA: 23696 seconds)\n",
            "cycle (3 / 100), batch (41 / 70) loss = (G: 7.17939425, D: 0.19290067) (3.783 sec/batch) (ETA: 23688 seconds)\n",
            "cycle (3 / 100), batch (42 / 70) loss = (G: 3.53550553, D: 0.52337122) (3.523 sec/batch) (ETA: 23597 seconds)\n",
            "cycle (3 / 100), batch (43 / 70) loss = (G: 1.67903399, D: 0.25586015) (3.625 sec/batch) (ETA: 23632 seconds)\n",
            "cycle (3 / 100), batch (44 / 70) loss = (G: 6.50880003, D: 0.01868530) (3.763 sec/batch) (ETA: 23681 seconds)\n",
            "cycle (3 / 100), batch (45 / 70) loss = (G: 7.49184608, D: 0.05013701) (3.849 sec/batch) (ETA: 23716 seconds)\n",
            "cycle (3 / 100), batch (46 / 70) loss = (G: 6.74338198, D: 0.87001151) (3.763 sec/batch) (ETA: 23690 seconds)\n",
            "cycle (3 / 100), batch (47 / 70) loss = (G: 5.26111126, D: 0.18921457) (3.679 sec/batch) (ETA: 23662 seconds)\n",
            "cycle (3 / 100), batch (48 / 70) loss = (G: 4.67917156, D: 0.13483876) (3.725 sec/batch) (ETA: 23680 seconds)\n",
            "cycle (3 / 100), batch (49 / 70) loss = (G: 3.55206251, D: 0.10125268) (3.756 sec/batch) (ETA: 23693 seconds)\n",
            "cycle (3 / 100), batch (50 / 70) loss = (G: 4.35858345, D: 0.10903430) (3.741 sec/batch) (ETA: 23691 seconds)\n",
            "cycle (3 / 100), batch (51 / 70) loss = (G: 4.53485680, D: 0.05411948) (3.619 sec/batch) (ETA: 23648 seconds)\n",
            "cycle (3 / 100), batch (52 / 70) loss = (G: 5.29184246, D: 0.20845871) (3.551 sec/batch) (ETA: 23623 seconds)\n",
            "cycle (3 / 100), batch (53 / 70) loss = (G: 5.08027458, D: 0.10394124) (3.436 sec/batch) (ETA: 23579 seconds)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmcjOgILA7a7"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJh-EnPTB4si"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dnYbzUfAyFg"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}