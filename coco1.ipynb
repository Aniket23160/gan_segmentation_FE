{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coco1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/coco/blob/main/coco1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMZE-RHXWZHe",
        "outputId": "fc8808c6-dd9d-4ba5-f762-f1717ee6f2bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iJPhBoLrRQg7",
        "outputId": "b762f5ed-194f-46f6-df16-e5c76a440444"
      },
      "source": [
        "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html\n",
            "Collecting detectron2\n",
            "\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/detectron2-0.3%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 820kB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.4.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Collecting Pillow>=7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6d/b719ae8e21660a6a962636896dc4b7d657ef451a3ab941516401846ac5cb/Pillow-8.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.41.1)\n",
            "Collecting fvcore>=0.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/54/8dc13492b0c118c86c300c3850303bd65bad2e0baa5324bfc1ee7fecf540/fvcore-0.1.4.post20210325.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (54.1.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.22)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.27.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.0)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 25.3MB/s \n",
            "\u001b[?25hCollecting iopath<0.1.8,>=0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (3.7.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.4.post20210325-cp37-none-any.whl size=60464 sha256=a55bec37c63807bc595002b65f15cd455af6ce4550ae6831dff58e0349e58208\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/cc/60/336d5f2691ba7f6e0d5c026f2cf50992b75a975bbd5c1d856c\n",
            "Successfully built fvcore\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow, pyyaml, yacs, portalocker, iopath, fvcore, detectron2\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Pillow-8.1.2 detectron2-0.3+cu101 fvcore-0.1.4.post20210325 iopath-0.1.7 portalocker-2.2.1 pyyaml-5.4.1 yacs-0.1.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEyiubt0exdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646ffa04-1080-44ba-9cca-7ea4e667b680"
      },
      "source": [
        "!git clone https://github.com/fanq15/FewX"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FewX'...\n",
            "remote: Enumerating objects: 177, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 177 (delta 70), reused 106 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (177/177), 4.75 MiB | 12.02 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVDGjgrZW3G7"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/MyDrive/coco/annotations_trainval2017.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuSiosYiTacw",
        "outputId": "3059a639-2bd5-4c76-8108-fe6c2158c98f"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from os.path import join, isdir\n",
        "from os import mkdir, makedirs\n",
        "from concurrent import futures\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "\n",
        "\n",
        "def filter_coco(coco, cls_split):\n",
        "    new_anns = []\n",
        "    all_cls_dict = {}\n",
        "    for img_id, id in enumerate(coco.imgs):\n",
        "        img = coco.loadImgs(id)[0]\n",
        "        anns = coco.loadAnns(coco.getAnnIds(imgIds=id, iscrowd=None))\n",
        "        skip_flag = False\n",
        "        img_cls_dict = {}\n",
        "        if len(anns) == 0:\n",
        "            continue\n",
        "        for ann in anns:\n",
        "            segmentation = ann['segmentation']\n",
        "            area = ann['area']\n",
        "            iscrowd = ann['iscrowd']\n",
        "            image_id = ann['image_id']\n",
        "            bbox = ann['bbox']\n",
        "            category_id = ann['category_id']\n",
        "            id = ann['id']\n",
        "            bbox_area = bbox[2] * bbox[3]\n",
        "            \n",
        "            # filter images with small boxes\n",
        "            if category_id in cls_split:\n",
        "                if bbox_area < 32 * 32:\n",
        "                    skip_flag = True\n",
        "                \n",
        "        if skip_flag:\n",
        "            continue\n",
        "        else:\n",
        "            for ann in anns:\n",
        "                category_id = ann['category_id']\n",
        "                if category_id in cls_split:\n",
        "                    new_anns.append(ann)\n",
        "\n",
        "                    if category_id in all_cls_dict.keys():\n",
        "                        all_cls_dict[category_id] += 1\n",
        "                    else:\n",
        "                        all_cls_dict[category_id] = 1\n",
        "                        \n",
        "    print(len(new_anns))\n",
        "    print(sorted(all_cls_dict.items(), key = lambda kv:(kv[1], kv[0])))     \n",
        "    return new_anns\n",
        "\n",
        "\n",
        "root_path = '/content/-f'\n",
        "print(root_path)\n",
        "#root_path = '/home/fanqi/data/COCO'\n",
        "dataDir = '/content/annotations'\n",
        "support_dict = {}\n",
        "\n",
        "support_dict['support_box'] = []\n",
        "support_dict['category_id'] = []\n",
        "support_dict['image_id'] = []\n",
        "support_dict['id'] = []\n",
        "support_dict['file_path'] = []\n",
        "\n",
        "voc_inds = (0, 1, 2, 3, 4, 5, 6, 8, 14, 15, 16, 17, 18, 19, 39, 56, 57, 58, 60, 62)\n",
        "\n",
        "\n",
        "for dataType in ['instances_train2017.json']: #, 'split_voc_instances_train2017.json']:\n",
        "    annFile = join(dataDir, dataType)\n",
        "\n",
        "    with open(annFile,'r') as load_f:\n",
        "        dataset = json.load(load_f)\n",
        "        print(dataset.keys())\n",
        "        save_info = dataset['info']\n",
        "        save_licenses = dataset['licenses']\n",
        "        save_images = dataset['images']\n",
        "        save_categories = dataset['categories']\n",
        "        save_annotations = dataset['annotations']\n",
        "\n",
        "\n",
        "    inds_split2 = [i for i in range(len(save_categories)) if i not in voc_inds]\n",
        "\n",
        "    # split annotations according to categories\n",
        "    categories_split1 = [save_categories[i] for i in voc_inds]\n",
        "    categories_split2 = [save_categories[i] for i in inds_split2]\n",
        "    cids_split1 = [c['id'] for c in categories_split1]\n",
        "    cids_split2 = [c['id'] for c in categories_split2]\n",
        "    print('Split 1: {} classes'.format(len(categories_split1)))\n",
        "    for c in categories_split1:\n",
        "        print('\\t', c['name'])\n",
        "    print('Split 2: {} classes'.format(len(categories_split2)))\n",
        "    for c in categories_split2:\n",
        "        print('\\t', c['name'])\n",
        "    \n",
        "    coco = COCO(annFile)\n",
        "\n",
        "    # for non-voc, there can be non_voc images\n",
        "    annotations_split2 = filter_coco(coco, cids_split2)\n",
        "\n",
        "    # for voc, there can be non_voc images\n",
        "    annotations = dataset['annotations']\n",
        "    annotations_split1 = []\n",
        "    \n",
        "    for ann in annotations:\n",
        "        if ann['category_id'] in cids_split1: # voc 20\n",
        "            annotations_split1.append(ann)\n",
        "\n",
        "    dataset_split1 = {\n",
        "        'info': save_info,\n",
        "        'licenses': save_licenses,\n",
        "        'images': save_images,\n",
        "        'annotations': annotations_split1,\n",
        "        'categories': save_categories}\n",
        "    dataset_split2 = {\n",
        "        'info': save_info,\n",
        "        'licenses': save_licenses,\n",
        "        'images': save_images,\n",
        "        'annotations': annotations_split2,\n",
        "        'categories': save_categories}\n",
        "    new_annotations_path = os.path.join(root_path, 'new_annotations')\n",
        "    if not os.path.exists(new_annotations_path):\n",
        "        os.makedirs(new_annotations_path)\n",
        "    split2_file = os.path.join(root_path, 'new_annotations/final_split_non_voc_instances_train2017.json')\n",
        "\n",
        "    with open(split2_file, 'w') as f:\n",
        "        json.dump(dataset_split2, f) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/-f\n",
            "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])\n",
            "Split 1: 20 classes\n",
            "\t person\n",
            "\t bicycle\n",
            "\t car\n",
            "\t motorcycle\n",
            "\t airplane\n",
            "\t bus\n",
            "\t train\n",
            "\t boat\n",
            "\t bird\n",
            "\t cat\n",
            "\t dog\n",
            "\t horse\n",
            "\t sheep\n",
            "\t cow\n",
            "\t bottle\n",
            "\t chair\n",
            "\t couch\n",
            "\t potted plant\n",
            "\t dining table\n",
            "\t tv\n",
            "Split 2: 60 classes\n",
            "\t truck\n",
            "\t traffic light\n",
            "\t fire hydrant\n",
            "\t stop sign\n",
            "\t parking meter\n",
            "\t bench\n",
            "\t elephant\n",
            "\t bear\n",
            "\t zebra\n",
            "\t giraffe\n",
            "\t backpack\n",
            "\t umbrella\n",
            "\t handbag\n",
            "\t tie\n",
            "\t suitcase\n",
            "\t frisbee\n",
            "\t skis\n",
            "\t snowboard\n",
            "\t sports ball\n",
            "\t kite\n",
            "\t baseball bat\n",
            "\t baseball glove\n",
            "\t skateboard\n",
            "\t surfboard\n",
            "\t tennis racket\n",
            "\t wine glass\n",
            "\t cup\n",
            "\t fork\n",
            "\t knife\n",
            "\t spoon\n",
            "\t bowl\n",
            "\t banana\n",
            "\t apple\n",
            "\t sandwich\n",
            "\t orange\n",
            "\t broccoli\n",
            "\t carrot\n",
            "\t hot dog\n",
            "\t pizza\n",
            "\t donut\n",
            "\t cake\n",
            "\t bed\n",
            "\t toilet\n",
            "\t laptop\n",
            "\t mouse\n",
            "\t remote\n",
            "\t keyboard\n",
            "\t cell phone\n",
            "\t microwave\n",
            "\t oven\n",
            "\t toaster\n",
            "\t sink\n",
            "\t refrigerator\n",
            "\t book\n",
            "\t clock\n",
            "\t vase\n",
            "\t scissors\n",
            "\t teddy bear\n",
            "\t hair drier\n",
            "\t toothbrush\n",
            "loading annotations into memory...\n",
            "Done (t=19.70s)\n",
            "creating index...\n",
            "index created!\n",
            "148472\n",
            "[(80, 73), (89, 119), (14, 511), (78, 671), (40, 749), (90, 783), (37, 848), (74, 918), (87, 978), (39, 997), (13, 1130), (11, 1172), (23, 1241), (82, 1273), (36, 1296), (38, 1331), (34, 1343), (79, 1346), (76, 1351), (75, 1482), (10, 1550), (43, 1694), (58, 1842), (50, 2037), (46, 2071), (53, 2248), (73, 2291), (48, 2499), (35, 2572), (77, 2608), (27, 2632), (32, 2679), (85, 2686), (49, 2744), (55, 2785), (81, 2859), (41, 2922), (54, 2975), (57, 2986), (86, 3001), (65, 3005), (31, 3029), (33, 3134), (42, 3385), (88, 3423), (61, 3440), (70, 3470), (28, 3659), (52, 3806), (84, 3849), (59, 3852), (15, 3979), (22, 4011), (24, 4100), (60, 4229), (56, 4363), (25, 4481), (8, 4483), (51, 5209), (47, 6272)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYf0Z1oqc9xZ"
      },
      "source": [
        "    # from PIL import Image\n",
        "    # import urllib \n",
        "    # dataDir = '.'\n",
        "    # #root_path = '/home/fanqi/data/COCO'\n",
        "    # root_path = '/content/-f'\n",
        "    # support_path = os.path.join(root_path, 'support')\n",
        "    # if not isdir(support_path): \n",
        "    #     mkdir(support_path)\n",
        "    # #else:\n",
        "    # #    shutil.rmtree(support_path)\n",
        "\n",
        "    # support_dict = {}\n",
        "    \n",
        "    # support_dict['support_box'] = []\n",
        "    # support_dict['category_id'] = []\n",
        "    # support_dict['image_id'] = []\n",
        "    # support_dict['id'] = []\n",
        "    # support_dict['file_path'] = []\n",
        "\n",
        "    # for dataType in ['train2017']: #, 'train2017']:\n",
        "    #     set_crop_base_path = join(support_path, dataType)\n",
        "    #     set_img_base_path = join(dataDir, dataType)\n",
        "\n",
        "    #     annFile = os.path.join(root_path, 'new_annotations/final_split_non_voc_instances_train2017.json')\n",
        "    #     with open(annFile,'r') as load_f:\n",
        "    #         dataset = json.load(load_f)\n",
        "    #         print(dataset.keys())\n",
        "    #         save_info = dataset['info']\n",
        "    #         save_licenses = dataset['licenses']\n",
        "    #         save_images = dataset['images']\n",
        "    #         save_categories = dataset['categories']\n",
        "\n",
        "    #     coco = COCO(annFile)\n",
        "        \n",
        "    #     for img_id, id in enumerate(coco.imgs):\n",
        "    #         if img_id % 100 == 0:\n",
        "    #             print(img_id)\n",
        "    #         img = coco.loadImgs(id)[0]\n",
        "    #         anns = coco.loadAnns(coco.getAnnIds(imgIds=id, iscrowd=None))\n",
        "\n",
        "    #         if len(anns) == 0:\n",
        "    #             continue\n",
        "\n",
        "    #         frame_crop_base_path = join(set_crop_base_path, img['file_name'].split('/')[-1].split('.')[0])\n",
        "    #         if not isdir(frame_crop_base_path): makedirs(frame_crop_base_path)\n",
        "    #         print('{}/{}'.format(set_img_base_path, img['file_name']))\n",
        "    #         im = np.array(Image.open(urllib.request.urlopen(img['coco_url'])))\n",
        "    #         #print('{}/{}'.format(set_img_base_path, img['file_name']))\n",
        "    #         for item_id, ann in enumerate(anns):\n",
        "    #             rect = ann['bbox']\n",
        "    #             bbox = [rect[0], rect[1], rect[0] + rect[2], rect[1] + rect[3]]\n",
        "    #             support_img, support_box = crop_support(im, bbox)\n",
        "    #             #im_name = img['file_name'].split('.')[0] + '_' + str(item_id) + '.jpg'\n",
        "    #             #output_dir = './fig'\n",
        "    #             #vis_image(support_img[:, :, ::-1], support_box, join(frame_crop_base_path, '{:04d}.jpg'.format(item_id)))\n",
        "    #             if rect[2] <= 0 or rect[3] <=0:\n",
        "    #                 print(rect)\n",
        "    #                 continue\n",
        "    #             file_path = join(frame_crop_base_path, '{:04d}.jpg'.format(item_id))\n",
        "    #             cv2.imwrite(file_path, support_img)\n",
        "    #             #print(file_path)\n",
        "    #             support_dict['support_box'].append(support_box.tolist())\n",
        "    #             support_dict['category_id'].append(ann['category_id'])\n",
        "    #             support_dict['image_id'].append(ann['image_id'])\n",
        "    #             support_dict['id'].append(ann['id'])\n",
        "    #             support_dict['file_path'].append(file_path)\n",
        "\n",
        "    #     support_df = pd.DataFrame.from_dict(support_dict)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOppIr__ZUNO"
      },
      "source": [
        "# from pycocotools.coco import COCO\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# from os.path import join, isdir\n",
        "# from os import mkdir, makedirs\n",
        "# from concurrent import futures\n",
        "# import sys\n",
        "# import time\n",
        "# import math\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import json\n",
        "# import shutil\n",
        "# import sys\n",
        "# from PIL import Image\n",
        "# import urllib \n",
        "# def vis_image(im, bboxs, im_name):\n",
        "#     dpi = 300\n",
        "#     fig, ax = plt.subplots() \n",
        "#     ax.imshow(im, aspect='equal') \n",
        "#     plt.axis('off') \n",
        "#     height, width, channels = im.shape \n",
        "#     fig.set_size_inches(width/100.0/3.0, height/100.0/3.0) \n",
        "#     plt.gca().xaxis.set_major_locator(plt.NullLocator()) \n",
        "#     plt.gca().yaxis.set_major_locator(plt.NullLocator()) \n",
        "#     plt.subplots_adjust(top=1,bottom=0,left=0,right=1,hspace=0,wspace=0) \n",
        "#     plt.margins(0,0)\n",
        "#     # Show box (off by default, box_alpha=0.0)\n",
        "#     for bbox in bboxs:\n",
        "#         ax.add_patch(\n",
        "#             plt.Rectangle((bbox[0], bbox[1]),\n",
        "#                           bbox[2] - bbox[0],\n",
        "#                           bbox[3] - bbox[1],\n",
        "#                           fill=False, edgecolor='r',\n",
        "#                           linewidth=0.5, alpha=1))\n",
        "#     output_name = os.path.basename(im_name)\n",
        "#     plt.savefig(im_name, dpi=dpi, bbox_inches='tight', pad_inches=0)\n",
        "#     plt.close('all')\n",
        "\n",
        "\n",
        "# def crop_support(img, bbox):\n",
        "#     image_shape = img.shape[:2]  # h, w\n",
        "#     data_height, data_width = image_shape\n",
        "    \n",
        "#     img = img.transpose(2, 0, 1)\n",
        "\n",
        "#     x1 = int(bbox[0])\n",
        "#     y1 = int(bbox[1])\n",
        "#     x2 = int(bbox[2])\n",
        "#     y2 = int(bbox[3])\n",
        "    \n",
        "#     width = x2 - x1\n",
        "#     height = y2 - y1\n",
        "#     context_pixel = 16 #int(16 * im_scale)\n",
        "    \n",
        "#     new_x1 = 0\n",
        "#     new_y1 = 0\n",
        "#     new_x2 = width\n",
        "#     new_y2 = height\n",
        "#     target_size = (320, 320) #(384, 384)\n",
        " \n",
        "#     if width >= height:\n",
        "#         crop_x1 = x1 - context_pixel\n",
        "#         crop_x2 = x2 + context_pixel\n",
        "   \n",
        "#         # New_x1 and new_x2 will change when crop context or overflow\n",
        "#         new_x1 = new_x1 + context_pixel\n",
        "#         new_x2 = new_x1 + width\n",
        "#         if crop_x1 < 0:\n",
        "#             new_x1 = new_x1 + crop_x1\n",
        "#             new_x2 = new_x1 + width\n",
        "#             crop_x1 = 0\n",
        "#         if crop_x2 > data_width:\n",
        "#             crop_x2 = data_width\n",
        "            \n",
        "#         short_size = height\n",
        "#         long_size = crop_x2 - crop_x1\n",
        "#         y_center = int((y2+y1) / 2) #math.ceil((y2 + y1) / 2)\n",
        "#         crop_y1 = int(y_center - (long_size / 2)) #int(y_center - math.ceil(long_size / 2))\n",
        "#         crop_y2 = int(y_center + (long_size / 2)) #int(y_center + math.floor(long_size / 2))\n",
        "        \n",
        "#         # New_y1 and new_y2 will change when crop context or overflow\n",
        "#         new_y1 = new_y1 + math.ceil((long_size - short_size) / 2)\n",
        "#         new_y2 = new_y1 + height\n",
        "#         if crop_y1 < 0:\n",
        "#             new_y1 = new_y1 + crop_y1\n",
        "#             new_y2 = new_y1 + height\n",
        "#             crop_y1 = 0\n",
        "#         if crop_y2 > data_height:\n",
        "#             crop_y2 = data_height\n",
        "        \n",
        "#         crop_short_size = crop_y2 - crop_y1\n",
        "#         crop_long_size = crop_x2 - crop_x1\n",
        "#         square = np.zeros((3, crop_long_size, crop_long_size), dtype = np.uint8)\n",
        "#         delta = int((crop_long_size - crop_short_size) / 2) #int(math.ceil((crop_long_size - crop_short_size) / 2))\n",
        "#         square_y1 = delta\n",
        "#         square_y2 = delta + crop_short_size\n",
        "\n",
        "#         new_y1 = new_y1 + delta\n",
        "#         new_y2 = new_y2 + delta\n",
        "        \n",
        "#         crop_box = img[:, crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "#         square[:, square_y1:square_y2, :] = crop_box\n",
        "\n",
        "#         #show_square = np.zeros((crop_long_size, crop_long_size, 3))#, dtype=np.int16)\n",
        "#         #show_crop_box = original_img[crop_y1:crop_y2, crop_x1:crop_x2, :]\n",
        "#         #show_square[square_y1:square_y2, :, :] = show_crop_box\n",
        "#         #show_square = show_square.astype(np.int16)\n",
        "#     else:\n",
        "#         crop_y1 = y1 - context_pixel\n",
        "#         crop_y2 = y2 + context_pixel\n",
        "   \n",
        "#         # New_y1 and new_y2 will change when crop context or overflow\n",
        "#         new_y1 = new_y1 + context_pixel\n",
        "#         new_y2 = new_y1 + height\n",
        "#         if crop_y1 < 0:\n",
        "#             new_y1 = new_y1 + crop_y1\n",
        "#             new_y2 = new_y1 + height\n",
        "#             crop_y1 = 0\n",
        "#         if crop_y2 > data_height:\n",
        "#             crop_y2 = data_height\n",
        "            \n",
        "#         short_size = width\n",
        "#         long_size = crop_y2 - crop_y1\n",
        "#         x_center = int((x2 + x1) / 2) #math.ceil((x2 + x1) / 2)\n",
        "#         crop_x1 = int(x_center - (long_size / 2)) #int(x_center - math.ceil(long_size / 2))\n",
        "#         crop_x2 = int(x_center + (long_size / 2)) #int(x_center + math.floor(long_size / 2))\n",
        "\n",
        "#         # New_x1 and new_x2 will change when crop context or overflow\n",
        "#         new_x1 = new_x1 + math.ceil((long_size - short_size) / 2)\n",
        "#         new_x2 = new_x1 + width\n",
        "#         if crop_x1 < 0:\n",
        "#             new_x1 = new_x1 + crop_x1\n",
        "#             new_x2 = new_x1 + width\n",
        "#             crop_x1 = 0\n",
        "#         if crop_x2 > data_width:\n",
        "#             crop_x2 = data_width\n",
        "\n",
        "#         crop_short_size = crop_x2 - crop_x1\n",
        "#         crop_long_size = crop_y2 - crop_y1\n",
        "#         square = np.zeros((3, crop_long_size, crop_long_size), dtype = np.uint8)\n",
        "#         delta = int((crop_long_size - crop_short_size) / 2) #int(math.ceil((crop_long_size - crop_short_size) / 2))\n",
        "#         square_x1 = delta\n",
        "#         square_x2 = delta + crop_short_size\n",
        "\n",
        "#         new_x1 = new_x1 + delta\n",
        "#         new_x2 = new_x2 + delta\n",
        "#         crop_box = img[:, crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "#         square[:, :, square_x1:square_x2] = crop_box\n",
        "\n",
        "#         #show_square = np.zeros((crop_long_size, crop_long_size, 3)) #, dtype=np.int16)\n",
        "#         #show_crop_box = original_img[crop_y1:crop_y2, crop_x1:crop_x2, :]\n",
        "#         #show_square[:, square_x1:square_x2, :] = show_crop_box\n",
        "#         #show_square = show_square.astype(np.int16)\n",
        "#     #print(crop_y2 - crop_y1, crop_x2 - crop_x1, bbox, data_height, data_width)\n",
        "\n",
        "#     square = square.astype(np.float32, copy=False)\n",
        "#     square_scale = float(target_size[0]) / long_size\n",
        "#     square = square.transpose(1,2,0)\n",
        "#     square = cv2.resize(square, target_size, interpolation=cv2.INTER_LINEAR) # None, None, fx=square_scale, fy=square_scale, interpolation=cv2.INTER_LINEAR)\n",
        "#     #square = square.transpose(2,0,1)\n",
        "#     square = square.astype(np.uint8)\n",
        "\n",
        "#     new_x1 = int(new_x1 * square_scale)\n",
        "#     new_y1 = int(new_y1 * square_scale)\n",
        "#     new_x2 = int(new_x2 * square_scale)\n",
        "#     new_y2 = int(new_y2 * square_scale)\n",
        "\n",
        "#     # For test\n",
        "#     #show_square = cv2.resize(show_square, target_size, interpolation=cv2.INTER_LINEAR) # None, None, fx=square_scale, fy=square_scale, interpolation=cv2.INTER_LINEAR)\n",
        "#     #self.vis_image(show_square, [new_x1, new_y1, new_x2, new_y2], img_path.split('/')[-1][:-4]+'_crop.jpg', './test')\n",
        "\n",
        "#     support_data = square\n",
        "#     support_box = np.array([new_x1, new_y1, new_x2, new_y2]).astype(np.float32)\n",
        "#     return support_data, support_box\n",
        "        \n",
        "\n",
        "# def main():\n",
        "#     dataDir = '/content/gdrive/MyDrive/coco'\n",
        "\n",
        "#     #root_path = '/home/fanqi/data/COCO'\n",
        "#     root_path = sys.argv[1]\n",
        "#     support_path = os.path.join(root_path, '10_shot_support')\n",
        "#     #support_path = '10_shot_support'\n",
        "#     if not isdir(support_path): \n",
        "#         mkdir(support_path)\n",
        "#     #else:\n",
        "#     #    shutil.rmtree(support_path)\n",
        "\n",
        "#     support_dict = {}\n",
        "    \n",
        "#     support_dict['support_box'] = []\n",
        "#     support_dict['category_id'] = []\n",
        "#     support_dict['image_id'] = []\n",
        "#     support_dict['id'] = []\n",
        "#     support_dict['file_path'] = []\n",
        "\n",
        "#     for dataType in ['train2017']: #, 'train2017']:\n",
        "#         set_crop_base_path = join(support_path, dataType)\n",
        "#         set_img_base_path = join(dataDir, dataType)\n",
        "\n",
        "#         # other information\n",
        "#         #annFile = '{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
        "#         #annFile = './new_annotations/final_split_voc_10_shot_instances_train2017.json'\n",
        "        \n",
        "#         annFile = '/content/FewX/datasets/coco/new_annotations/final_split_voc_10_shot_instances_train2017.json'\n",
        "        \n",
        "#         with open(annFile,'r') as load_f:\n",
        "#             dataset = json.load(load_f)\n",
        "#             print(dataset.keys())\n",
        "#             save_info = dataset['info']\n",
        "#             save_licenses = dataset['licenses']\n",
        "#             save_images = dataset['images']\n",
        "#             save_categories = dataset['categories']\n",
        "\n",
        "#         coco = COCO(annFile)\n",
        "        \n",
        "#         for img_id, id in enumerate(coco.imgs):\n",
        "#             if img_id % 100 == 0:\n",
        "#                 print(img_id)\n",
        "#             img = coco.loadImgs(id)[0]\n",
        "#             anns = coco.loadAnns(coco.getAnnIds(imgIds=id, iscrowd=None))\n",
        "\n",
        "#             if len(anns) == 0:\n",
        "#                 continue\n",
        "\n",
        "#             #print(img['file_name'])\n",
        "#             frame_crop_base_path = join(set_crop_base_path, img['file_name'].split('/')[-1].split('.')[0])\n",
        "#             if not isdir(frame_crop_base_path): makedirs(frame_crop_base_path)\n",
        "#             frame_crop_base_path='{}/{}'.format(set_img_base_path, img['file_name'].split('.')[0])\n",
        "#             path=os.path.join(frame_crop_base_path, '{:04d}.jpg'.format(img_id))\n",
        "#             im = np.array(Image.open(urllib.request.urlopen(img['coco_url'])))\n",
        "\n",
        "#             for item_id, ann in enumerate(anns):\n",
        "#                 #print(ann)\n",
        "#                 rect = ann['bbox']\n",
        "#                 bbox = [rect[0], rect[1], rect[0] + rect[2], rect[1] + rect[3]]\n",
        "#                 support_img, support_box = crop_support(im, bbox)\n",
        "#                 #im_name = img['file_name'].split('.')[0] + '_' + str(item_id) + '.jpg'\n",
        "#                 #output_dir = './fig'\n",
        "#                 #vis_image(support_img[:, :, ::-1], support_box, join(frame_crop_base_path, '{:04d}.jpg'.format(item_id)))\n",
        "#                 if rect[2] <= 0 or rect[3] <=0:\n",
        "#                     print(rect)\n",
        "#                     continue\n",
        "#                 file_path = join(frame_crop_base_path, '{:04d}.jpg'.format(item_id))\n",
        "#                 print(file_path)\n",
        "#                 cv2.imwrite(file_path, support_img)\n",
        "#                 #print(file_path)\n",
        "#                 support_dict['support_box'].append(support_box.tolist())\n",
        "#                 support_dict['category_id'].append(ann['category_id'])\n",
        "#                 support_dict['image_id'].append(ann['image_id'])\n",
        "#                 support_dict['id'].append(ann['id'])\n",
        "#                 support_dict['file_path'].append(file_path)\n",
        "\n",
        "#         support_df = pd.DataFrame.from_dict(support_dict)\n",
        "        \n",
        "#     return support_df\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     since = time.time()\n",
        "#     support_df = main()\n",
        "#     support_df.to_pickle(\"./10_shot_support_df.pkl\")\n",
        "\n",
        "#     time_elapsed = time.time() - since\n",
        "#     print('Total complete in {:.0f}m {:.0f}s'.format(\n",
        "#         time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqFUm5L7dmk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0460646-7ed2-426c-e4f6-45b25ff70b57"
      },
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-lexvzou6\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-lexvzou6\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (1.1.0)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (8.1.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (3.2.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (4.41.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (2.4.1)\n",
            "Requirement already satisfied: fvcore<0.1.5,>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (0.1.4.post20210325)\n",
            "Requirement already satisfied: iopath<0.1.8,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (0.1.7)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (2.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4) (1.3.0)\n",
            "Collecting omegaconf==2.1.0.dev22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1c/6ecce6df0e112f381c355144b4f8de56157e68abbf93a7dd3fbf98d28ac2/omegaconf-2.1.0.dev22-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs>=0.1.6->detectron2==0.4) (5.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4) (2.4.7)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (1.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (1.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (54.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4) (0.10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.8,>=0.1.7->detectron2==0.4) (2.2.1)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.4) (0.29.22)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.4) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.4) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.4) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.4) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.4) (3.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard->detectron2==0.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.4) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2==0.4) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2==0.4) (3.7.4.3)\n",
            "Building wheels for collected packages: detectron2, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.4-cp37-cp37m-linux_x86_64.whl size=5035468 sha256=821adf9695d015786ba4919c3d1ed46a68b501432ed8d32e118696f25a143564\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3fb4ucbs/wheels/33/ac/bb/5ef90585c21c67e2f0b6aae55ec6b43017ad57af33d5f4c339\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=c232526054c06c37211c7dac7f94fb3504c7071e951c4f0587ac72832343f19f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built detectron2 antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, detectron2\n",
            "  Found existing installation: detectron2 0.3+cu101\n",
            "    Uninstalling detectron2-0.3+cu101:\n",
            "      Successfully uninstalled detectron2-0.3+cu101\n",
            "Successfully installed antlr4-python3-runtime-4.8 detectron2-0.4 omegaconf-2.1.0.dev22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUxzxcE9hSaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6afad4-9201-47ec-d1af-670d7d7c41c8"
      },
      "source": [
        "%%writefile a.py\n",
        "from detectron2.config import CfgNode\n",
        "from detectron2.config.defaults import _C\n",
        "from detectron2.config import CfgNode as CN\n",
        "\n",
        "\n",
        "_C.SOLVER.HEAD_LR_FACTOR = 1.0\n",
        "\n",
        "_C.INPUT.FS = CN()\n",
        "_C.INPUT.FS.FEW_SHOT = False\n",
        "_C.INPUT.FS.SUPPORT_WAY = 2\n",
        "_C.INPUT.FS.SUPPORT_SHOT = 10\n",
        "\n",
        "def get_cfg() -> CfgNode:\n",
        "    \"\"\"\n",
        "    Get a copy of the default config.\n",
        "    Returns:\n",
        "        a detectron2 CfgNode instance.\n",
        "    \"\"\"\n",
        "\n",
        "    return _C.clone()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing a.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kluN9iQ4hbM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6a61af-0434-4106-af8d-2b604b50c5d7"
      },
      "source": [
        "%%writefile c.py\n",
        "import bisect\n",
        "import copy\n",
        "import itertools\n",
        "import logging\n",
        "import numpy as np\n",
        "import operator\n",
        "import pickle\n",
        "import torch.utils.data\n",
        "from fvcore.common.file_io import PathManager\n",
        "from tabulate import tabulate\n",
        "from termcolor import colored\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.utils.comm import get_world_size\n",
        "from detectron2.utils.env import seed_all_rng\n",
        "from detectron2.utils.logger import log_first_n\n",
        "\n",
        "from detectron2.data.catalog import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.common import AspectRatioGroupedDataset, DatasetFromList, MapDataset\n",
        "from detectron2.data.dataset_mapper import DatasetMapper\n",
        "from detectron2.data.detection_utils import check_metadata_consistency\n",
        "from detectron2.data.samplers import InferenceSampler, RepeatFactorTrainingSampler, TrainingSampler\n",
        "\n",
        "from detectron2.data.build import build_batch_data_loader, filter_images_with_only_crowd_annotations, load_proposals_into_dataset, filter_images_with_few_keypoints, print_instances_class_histogram, trivial_batch_collator, get_detection_dataset_dicts\n",
        "\n",
        "def fsod_get_detection_dataset_dicts(\n",
        "    dataset_names, filter_empty=True, min_keypoints=0, proposal_files=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Load and prepare dataset dicts for instance detection/segmentation and semantic segmentation.\n",
        "    Args:\n",
        "        dataset_names (list[str]): a list of dataset names\n",
        "        filter_empty (bool): whether to filter out images without instance annotations\n",
        "        min_keypoints (int): filter out images with fewer keypoints than\n",
        "            `min_keypoints`. Set to 0 to do nothing.\n",
        "        proposal_files (list[str]): if given, a list of object proposal files\n",
        "            that match each dataset in `dataset_names`.\n",
        "    \"\"\"\n",
        "    assert len(dataset_names)\n",
        "    dataset_dicts_original = [DatasetCatalog.get(dataset_name) for dataset_name in dataset_names]\n",
        "    for dataset_name, dicts in zip(dataset_names, dataset_dicts_original):\n",
        "        assert len(dicts), \"Dataset '{}' is empty!\".format(dataset_name)\n",
        "\n",
        "    if proposal_files is not None:\n",
        "        assert len(dataset_names) == len(proposal_files)\n",
        "        # load precomputed proposals from proposal files\n",
        "        dataset_dicts_original = [\n",
        "            load_proposals_into_dataset(dataset_i_dicts, proposal_file)\n",
        "            for dataset_i_dicts, proposal_file in zip(dataset_dicts_original, proposal_files)\n",
        "        ]\n",
        "\n",
        "    if 'train' not in dataset_names[0]:\n",
        "        dataset_dicts = list(itertools.chain.from_iterable(dataset_dicts_original))\n",
        "    else:\n",
        "        dataset_dicts_original = list(itertools.chain.from_iterable(dataset_dicts_original))\n",
        "        dataset_dicts_original = filter_images_with_only_crowd_annotations(dataset_dicts_original)\n",
        "        ###################################################################################\n",
        "        # split image-based annotations to instance-based annotations for few-shot learning\n",
        "        dataset_dicts = []\n",
        "        index_dicts = []\n",
        "        split_flag = True\n",
        "        if split_flag:\n",
        "            for record in dataset_dicts_original:\n",
        "                file_name = record['file_name']\n",
        "                height = record['height']\n",
        "                width = record['width']\n",
        "                image_id = record['image_id']\n",
        "                annotations = record['annotations']\n",
        "                category_dict = {}\n",
        "                for ann_id, ann in enumerate(annotations):\n",
        "\n",
        "                    ann.pop(\"segmentation\", None)\n",
        "                    ann.pop(\"keypoints\", None)\n",
        "\n",
        "                    category_id = ann['category_id']\n",
        "                    if category_id not in category_dict.keys():\n",
        "                        category_dict[category_id] = [ann]\n",
        "                    else:\n",
        "                        category_dict[category_id].append(ann)\n",
        "                \n",
        "                for key, item in category_dict.items():\n",
        "                    instance_ann = {}\n",
        "                    instance_ann['file_name'] = file_name\n",
        "                    instance_ann['height'] = height\n",
        "                    instance_ann['width'] = width\n",
        "\n",
        "                    instance_ann['annotations'] = item\n",
        "                    \n",
        "                    dataset_dicts.append(instance_ann)\n",
        "\n",
        "\n",
        "    has_instances = \"annotations\" in dataset_dicts[0]\n",
        "    # Keep images without instance-level GT if the dataset has semantic labels.\n",
        "    if filter_empty and has_instances and \"sem_seg_file_name\" not in dataset_dicts[0]:\n",
        "        dataset_dicts = filter_images_with_only_crowd_annotations(dataset_dicts)\n",
        "\n",
        "    if min_keypoints > 0 and has_instances:\n",
        "        dataset_dicts = filter_images_with_few_keypoints(dataset_dicts, min_keypoints)\n",
        "\n",
        "    if has_instances:\n",
        "        try:\n",
        "            class_names = MetadataCatalog.get(dataset_names[0]).thing_classes\n",
        "            check_metadata_consistency(\"thing_classes\", dataset_names)\n",
        "            print_instances_class_histogram(dataset_dicts, class_names)\n",
        "        except AttributeError:  # class names are not available for this dataset\n",
        "            pass\n",
        "    return dataset_dicts\n",
        "\n",
        "def build_detection_train_loader(cfg, mapper=None):\n",
        "    \"\"\"\n",
        "    A data loader is created by the following steps:\n",
        "    1. Use the dataset names in config to query :class:`DatasetCatalog`, and obtain a list of dicts.\n",
        "    2. Coordinate a random shuffle order shared among all processes (all GPUs)\n",
        "    3. Each process spawn another few workers to process the dicts. Each worker will:\n",
        "       * Map each metadata dict into another format to be consumed by the model.\n",
        "       * Batch them by simply putting dicts into a list.\n",
        "    The batched ``list[mapped_dict]`` is what this dataloader will yield.\n",
        "    Args:\n",
        "        cfg (CfgNode): the config\n",
        "        mapper (callable): a callable which takes a sample (dict) from dataset and\n",
        "            returns the format to be consumed by the model.\n",
        "            By default it will be `DatasetMapper(cfg, True)`.\n",
        "    Returns:\n",
        "        an infinite iterator of training data\n",
        "    \"\"\"\n",
        "    dataset_dicts = fsod_get_detection_dataset_dicts(\n",
        "        cfg.DATASETS.TRAIN,\n",
        "        filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
        "        min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
        "        if cfg.MODEL.KEYPOINT_ON\n",
        "        else 0,\n",
        "        proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,\n",
        "    )\n",
        "    dataset = DatasetFromList(dataset_dicts, copy=False)\n",
        "    \n",
        "    if mapper is None:\n",
        "        mapper = DatasetMapper(cfg, True)\n",
        "    dataset = MapDataset(dataset, mapper)\n",
        "\n",
        "    sampler_name = cfg.DATALOADER.SAMPLER_TRAIN\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Using training sampler {}\".format(sampler_name))\n",
        "    # TODO avoid if-else?\n",
        "    if sampler_name == \"TrainingSampler\":\n",
        "        sampler = TrainingSampler(len(dataset))\n",
        "    elif sampler_name == \"RepeatFactorTrainingSampler\":\n",
        "        repeat_factors = RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(\n",
        "            dataset_dicts, cfg.DATALOADER.REPEAT_THRESHOLD\n",
        "        )\n",
        "        sampler = RepeatFactorTrainingSampler(repeat_factors)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown training sampler: {}\".format(sampler_name))\n",
        "    return build_batch_data_loader(\n",
        "        dataset,\n",
        "        sampler,\n",
        "        cfg.SOLVER.IMS_PER_BATCH,\n",
        "        aspect_ratio_grouping=cfg.DATALOADER.ASPECT_RATIO_GROUPING,\n",
        "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
        "    )\n",
        "\n",
        "def build_detection_test_loader(cfg, dataset_name, mapper=None):\n",
        "    \"\"\"\n",
        "    Similar to `build_detection_train_loader`.\n",
        "    But this function uses the given `dataset_name` argument (instead of the names in cfg),\n",
        "    and uses batch size 1.\n",
        "    Args:\n",
        "        cfg: a detectron2 CfgNode\n",
        "        dataset_name (str): a name of the dataset that's available in the DatasetCatalog\n",
        "        mapper (callable): a callable which takes a sample (dict) from dataset\n",
        "           and returns the format to be consumed by the model.\n",
        "           By default it will be `DatasetMapper(cfg, False)`.\n",
        "    Returns:\n",
        "        DataLoader: a torch DataLoader, that loads the given detection\n",
        "        dataset, with test-time transformation and batching.\n",
        "    \"\"\"\n",
        "    dataset_dicts = get_detection_dataset_dicts(\n",
        "        [dataset_name],\n",
        "        filter_empty=False, # True,\n",
        "        proposal_files=[\n",
        "            cfg.DATASETS.PROPOSAL_FILES_TEST[list(cfg.DATASETS.TEST).index(dataset_name)]\n",
        "        ]\n",
        "        if cfg.MODEL.LOAD_PROPOSALS\n",
        "        else None,\n",
        "    )\n",
        "\n",
        "    dataset = DatasetFromList(dataset_dicts)\n",
        "    if mapper is None:\n",
        "        mapper = DatasetMapper(cfg, False) # True)\n",
        "    dataset = MapDataset(dataset, mapper)\n",
        "\n",
        "    sampler = InferenceSampler(len(dataset))\n",
        "    # Always use 1 image per worker during inference since this is the\n",
        "    # standard when reporting inference time in papers.\n",
        "    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, 1, drop_last=False)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
        "        batch_sampler=batch_sampler,\n",
        "        collate_fn=trivial_batch_collator,\n",
        "    )\n",
        "    return data_loader"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing c.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tClmaBgwhjFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe3b9a9-b748-4145-a32e-273af61e89fd"
      },
      "source": [
        "%%writefile b.py\n",
        "import copy\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from fvcore.common.file_io import PathManager\n",
        "from PIL import Image\n",
        "\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.data import transforms as T\n",
        "\n",
        "import pandas as pd\n",
        "from detectron2.data.catalog import MetadataCatalog\n",
        "\n",
        "\"\"\"\n",
        "This file contains the default mapping that's applied to \"dataset dicts\".\n",
        "\"\"\"\n",
        "\n",
        "__all__ = [\"DatasetMapperWithSupport\"]\n",
        "\n",
        "\n",
        "class DatasetMapperWithSupport:\n",
        "    \"\"\"\n",
        "    A callable which takes a dataset dict in Detectron2 Dataset format,\n",
        "    and map it into a format used by the model.\n",
        "    This is the default callable to be used to map your dataset dict into training data.\n",
        "    You may need to follow it to implement your own one for customized logic,\n",
        "    such as a different way to read or transform images.\n",
        "    See :doc:`/tutorials/data_loading` for details.\n",
        "    The callable currently does the following:\n",
        "    1. Read the image from \"file_name\"\n",
        "    2. Applies cropping/geometric transforms to the image and annotations\n",
        "    3. Prepare data and annotations to Tensor and :class:`Instances`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, is_train=True):\n",
        "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
        "            self.crop_gen = T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)\n",
        "            logging.getLogger(__name__).info(\"CropGen used in training: \" + str(self.crop_gen))\n",
        "        else:\n",
        "            self.crop_gen = None\n",
        "\n",
        "        self.tfm_gens = utils.build_transform_gen(cfg, is_train)\n",
        "\n",
        "        # fmt: off\n",
        "        self.img_format     = cfg.INPUT.FORMAT\n",
        "        self.mask_on        = cfg.MODEL.MASK_ON\n",
        "        self.mask_format    = cfg.INPUT.MASK_FORMAT\n",
        "        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON\n",
        "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
        "\n",
        "        self.few_shot       = cfg.INPUT.FS.FEW_SHOT\n",
        "        self.support_way       = cfg.INPUT.FS.SUPPORT_WAY\n",
        "        self.support_shot       = cfg.INPUT.FS.SUPPORT_SHOT\n",
        "        # fmt: on\n",
        "        if self.keypoint_on and is_train:\n",
        "            # Flip only makes sense in training\n",
        "            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)\n",
        "        else:\n",
        "            self.keypoint_hflip_indices = None\n",
        "\n",
        "        if self.load_proposals:\n",
        "            self.proposal_min_box_size = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE\n",
        "            self.proposal_topk = (\n",
        "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
        "                if is_train\n",
        "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
        "            )\n",
        "        self.is_train = is_train\n",
        "\n",
        "        if self.is_train:\n",
        "            # support_df\n",
        "            self.support_on = True\n",
        "            if self.few_shot:\n",
        "                self.support_df = pd.read_pickle(\"./datasets/coco/10_shot_support_df.pkl\")\n",
        "            else:\n",
        "                self.support_df = pd.read_pickle(\"./datasets/coco/train_support_df.pkl\")\n",
        "\n",
        "            metadata = MetadataCatalog.get('coco_2017_train')\n",
        "            # unmap the category mapping ids for COCO\n",
        "            reverse_id_mapper = lambda dataset_id: metadata.thing_dataset_id_to_contiguous_id[dataset_id]  # noqa\n",
        "            self.support_df['category_id'] = self.support_df['category_id'].map(reverse_id_mapper)\n",
        "\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n",
        "        Returns:\n",
        "            dict: a format that builtin models in detectron2 accept\n",
        "        \"\"\"\n",
        "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
        "        # USER: Write your own image loading if it's not from a file\n",
        "        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
        "        utils.check_image_size(dataset_dict, image)\n",
        "        if self.is_train:\n",
        "            # support\n",
        "            if self.support_on:\n",
        "                if \"annotations\" in dataset_dict:\n",
        "                    # USER: Modify this if you want to keep them for some reason.\n",
        "                    for anno in dataset_dict[\"annotations\"]:\n",
        "                        if not self.mask_on:\n",
        "                            anno.pop(\"segmentation\", None)\n",
        "                        if not self.keypoint_on:\n",
        "                            anno.pop(\"keypoints\", None)\n",
        "                support_images, support_bboxes, support_cls = self.generate_support(dataset_dict)\n",
        "                dataset_dict['support_images'] = torch.as_tensor(np.ascontiguousarray(support_images))\n",
        "                dataset_dict['support_bboxes'] = support_bboxes\n",
        "                dataset_dict['support_cls'] = support_cls\n",
        "\n",
        "        if \"annotations\" not in dataset_dict:\n",
        "            image, transforms = T.apply_transform_gens(\n",
        "                ([self.crop_gen] if self.crop_gen else []) + self.tfm_gens, image\n",
        "            )\n",
        "        else:\n",
        "            # Crop around an instance if there are instances in the image.\n",
        "            # USER: Remove if you don't use cropping\n",
        "            if self.crop_gen:\n",
        "                crop_tfm = utils.gen_crop_transform_with_instance(\n",
        "                    self.crop_gen.get_crop_size(image.shape[:2]),\n",
        "                    image.shape[:2],\n",
        "                    np.random.choice(dataset_dict[\"annotations\"]),\n",
        "                )\n",
        "                image = crop_tfm.apply_image(image)\n",
        "            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n",
        "            if self.crop_gen:\n",
        "                transforms = crop_tfm + transforms\n",
        "\n",
        "        image_shape = image.shape[:2]  # h, w\n",
        "\n",
        "        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n",
        "        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n",
        "        # Therefore it's important to use torch.Tensor.\n",
        "        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
        "\n",
        "        # USER: Remove if you don't use pre-computed proposals.\n",
        "        # Most users would not need this feature.\n",
        "        if self.load_proposals:\n",
        "            utils.transform_proposals(\n",
        "                dataset_dict,\n",
        "                image_shape,\n",
        "                transforms,\n",
        "                self.proposal_min_box_size,\n",
        "                self.proposal_topk,\n",
        "            )\n",
        "\n",
        "        if not self.is_train:\n",
        "            # USER: Modify this if you want to keep them for some reason.\n",
        "            dataset_dict.pop(\"annotations\", None)\n",
        "            dataset_dict.pop(\"sem_seg_file_name\", None)\n",
        "            return dataset_dict\n",
        "\n",
        "        if \"annotations\" in dataset_dict:\n",
        "            # USER: Modify this if you want to keep them for some reason.\n",
        "            for anno in dataset_dict[\"annotations\"]:\n",
        "                if not self.mask_on:\n",
        "                    anno.pop(\"segmentation\", None)\n",
        "                if not self.keypoint_on:\n",
        "                    anno.pop(\"keypoints\", None)\n",
        "            \n",
        "            # USER: Implement additional transformations if you have other types of data\n",
        "            annos = [\n",
        "                utils.transform_instance_annotations(\n",
        "                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n",
        "                )\n",
        "                for obj in dataset_dict.pop(\"annotations\")\n",
        "                if obj.get(\"iscrowd\", 0) == 0\n",
        "            ]\n",
        "            instances = utils.annotations_to_instances(\n",
        "                annos, image_shape, mask_format=self.mask_format\n",
        "            )\n",
        "            # Create a tight bounding box from masks, useful when image is cropped\n",
        "            if self.crop_gen and instances.has(\"gt_masks\"):\n",
        "                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n",
        "            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
        "\n",
        "        # USER: Remove if you don't do semantic/panoptic segmentation.\n",
        "        if \"sem_seg_file_name\" in dataset_dict:\n",
        "            with PathManager.open(dataset_dict.pop(\"sem_seg_file_name\"), \"rb\") as f:\n",
        "                sem_seg_gt = Image.open(f)\n",
        "                sem_seg_gt = np.asarray(sem_seg_gt, dtype=\"uint8\")\n",
        "            sem_seg_gt = transforms.apply_segmentation(sem_seg_gt)\n",
        "            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n",
        "            dataset_dict[\"sem_seg\"] = sem_seg_gt\n",
        "        return dataset_dict\n",
        "\n",
        "    def generate_support(self, dataset_dict):\n",
        "        support_way = self.support_way #2\n",
        "        support_shot = self.support_shot #5\n",
        "        \n",
        "        id = dataset_dict['annotations'][0]['id']\n",
        "        query_cls = self.support_df.loc[self.support_df['id']==id, 'category_id'].tolist()[0] # they share the same category_id and image_id\n",
        "        query_img = self.support_df.loc[self.support_df['id']==id, 'image_id'].tolist()[0]\n",
        "        all_cls = self.support_df.loc[self.support_df['image_id']==query_img, 'category_id'].tolist()\n",
        "\n",
        "        # Crop support data and get new support box in the support data\n",
        "        support_data_all = np.zeros((support_way * support_shot, 3, 320, 320), dtype = np.float32)\n",
        "        support_box_all = np.zeros((support_way * support_shot, 4), dtype = np.float32)\n",
        "        used_image_id = [query_img]\n",
        "\n",
        "        used_id_ls = []\n",
        "        for item in dataset_dict['annotations']:\n",
        "            used_id_ls.append(item['id'])\n",
        "        #used_category_id = [query_cls]\n",
        "        used_category_id = list(set(all_cls))\n",
        "        support_category_id = []\n",
        "        mixup_i = 0\n",
        "\n",
        "        for shot in range(support_shot):\n",
        "            # Support image and box\n",
        "            support_id = self.support_df.loc[(self.support_df['category_id'] == query_cls) & (~self.support_df['image_id'].isin(used_image_id)) & (~self.support_df['id'].isin(used_id_ls)), 'id'].sample(random_state=id).tolist()[0]\n",
        "            support_cls = self.support_df.loc[self.support_df['id'] == support_id, 'category_id'].tolist()[0]\n",
        "            support_img = self.support_df.loc[self.support_df['id'] == support_id, 'image_id'].tolist()[0]\n",
        "            used_id_ls.append(support_id) \n",
        "            used_image_id.append(support_img)\n",
        "\n",
        "            support_db = self.support_df.loc[self.support_df['id'] == support_id, :]\n",
        "            assert support_db['id'].values[0] == support_id\n",
        "            \n",
        "            support_data = utils.read_image('./datasets/coco/' + support_db[\"file_path\"].tolist()[0], format=self.img_format)\n",
        "            support_data = torch.as_tensor(np.ascontiguousarray(support_data.transpose(2, 0, 1)))\n",
        "            support_box = support_db['support_box'].tolist()[0]\n",
        "            #print(support_data)\n",
        "            support_data_all[mixup_i] = support_data\n",
        "            support_box_all[mixup_i] = support_box\n",
        "            support_category_id.append(0) #support_cls)\n",
        "            mixup_i += 1\n",
        "\n",
        "        if support_way == 1:\n",
        "            pass\n",
        "        else:\n",
        "            for way in range(support_way-1):\n",
        "                other_cls = self.support_df.loc[(~self.support_df['category_id'].isin(used_category_id)), 'category_id'].drop_duplicates().sample(random_state=id).tolist()[0]\n",
        "                used_category_id.append(other_cls)\n",
        "                for shot in range(support_shot):\n",
        "                    # Support image and box\n",
        "\n",
        "                    support_id = self.support_df.loc[(self.support_df['category_id'] == other_cls) & (~self.support_df['image_id'].isin(used_image_id)) & (~self.support_df['id'].isin(used_id_ls)), 'id'].sample(random_state=id).tolist()[0]\n",
        "                     \n",
        "                    support_cls = self.support_df.loc[self.support_df['id'] == support_id, 'category_id'].tolist()[0]\n",
        "                    support_img = self.support_df.loc[self.support_df['id'] == support_id, 'image_id'].tolist()[0]\n",
        "\n",
        "                    used_id_ls.append(support_id) \n",
        "                    used_image_id.append(support_img)\n",
        "\n",
        "                    support_db = self.support_df.loc[self.support_df['id'] == support_id, :]\n",
        "                    assert support_db['id'].values[0] == support_id\n",
        "\n",
        "                    support_data = utils.read_image('./datasets/coco/' + support_db[\"file_path\"].tolist()[0], format=self.img_format)\n",
        "                    support_data = torch.as_tensor(np.ascontiguousarray(support_data.transpose(2, 0, 1)))\n",
        "                    support_box = support_db['support_box'].tolist()[0]\n",
        "                    support_data_all[mixup_i] = support_data\n",
        "                    support_box_all[mixup_i] = support_box\n",
        "                    support_category_id.append(1) #support_cls)\n",
        "                    mixup_i += 1\n",
        "        \n",
        "        return support_data_all, support_box_all, support_category_id"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing b.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olYFG1WWhtB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e286716f-8d81-4500-9702-98aa86237f32"
      },
      "source": [
        "%%writefile d.py\n",
        "from enum import Enum\n",
        "from typing import Any, Callable, Dict, Iterable, List, Set, Type, Union\n",
        "import torch\n",
        "\n",
        "from detectron2.config import CfgNode\n",
        "\n",
        "from detectron2.solver.lr_scheduler import WarmupCosineLR, WarmupMultiStepLR\n",
        "\n",
        "_GradientClipperInput = Union[torch.Tensor, Iterable[torch.Tensor]]\n",
        "_GradientClipper = Callable[[_GradientClipperInput], None]\n",
        "\n",
        "\n",
        "class GradientClipType(Enum):\n",
        "    VALUE = \"value\"\n",
        "    NORM = \"norm\"\n",
        "\n",
        "\n",
        "def _create_gradient_clipper(cfg: CfgNode) -> _GradientClipper:\n",
        "    \"\"\"\n",
        "    Creates gradient clipping closure to clip by value or by norm,\n",
        "    according to the provided config.\n",
        "    \"\"\"\n",
        "    cfg = cfg.clone()\n",
        "\n",
        "    def clip_grad_norm(p: _GradientClipperInput):\n",
        "        torch.nn.utils.clip_grad_norm_(p, cfg.CLIP_VALUE, cfg.NORM_TYPE)\n",
        "\n",
        "    def clip_grad_value(p: _GradientClipperInput):\n",
        "        torch.nn.utils.clip_grad_value_(p, cfg.CLIP_VALUE)\n",
        "\n",
        "    _GRADIENT_CLIP_TYPE_TO_CLIPPER = {\n",
        "        GradientClipType.VALUE: clip_grad_value,\n",
        "        GradientClipType.NORM: clip_grad_norm,\n",
        "    }\n",
        "    return _GRADIENT_CLIP_TYPE_TO_CLIPPER[GradientClipType(cfg.CLIP_TYPE)]\n",
        "\n",
        "\n",
        "def _generate_optimizer_class_with_gradient_clipping(\n",
        "    optimizer_type: Type[torch.optim.Optimizer], gradient_clipper: _GradientClipper\n",
        ") -> Type[torch.optim.Optimizer]:\n",
        "    \"\"\"\n",
        "    Dynamically creates a new type that inherits the type of a given instance\n",
        "    and overrides the `step` method to add gradient clipping\n",
        "    \"\"\"\n",
        "\n",
        "    def optimizer_wgc_step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                gradient_clipper(p)\n",
        "        super(type(self), self).step(closure)\n",
        "\n",
        "    OptimizerWithGradientClip = type(\n",
        "        optimizer_type.__name__ + \"WithGradientClip\",\n",
        "        (optimizer_type,),\n",
        "        {\"step\": optimizer_wgc_step},\n",
        "    )\n",
        "    return OptimizerWithGradientClip\n",
        "\n",
        "\n",
        "def maybe_add_gradient_clipping(\n",
        "    cfg: CfgNode, optimizer: torch.optim.Optimizer\n",
        ") -> torch.optim.Optimizer:\n",
        "    \"\"\"\n",
        "    If gradient clipping is enabled through config options, wraps the existing\n",
        "    optimizer instance of some type OptimizerType to become an instance\n",
        "    of the new dynamically created class OptimizerTypeWithGradientClip\n",
        "    that inherits OptimizerType and overrides the `step` method to\n",
        "    include gradient clipping.\n",
        "    Args:\n",
        "        cfg: CfgNode\n",
        "            configuration options\n",
        "        optimizer: torch.optim.Optimizer\n",
        "            existing optimizer instance\n",
        "    Return:\n",
        "        optimizer: torch.optim.Optimizer\n",
        "            either the unmodified optimizer instance (if gradient clipping is\n",
        "            disabled), or the same instance with adjusted __class__ to override\n",
        "            the `step` method and include gradient clipping\n",
        "    \"\"\"\n",
        "    if not cfg.SOLVER.CLIP_GRADIENTS.ENABLED:\n",
        "        return optimizer\n",
        "    grad_clipper = _create_gradient_clipper(cfg.SOLVER.CLIP_GRADIENTS)\n",
        "    OptimizerWithGradientClip = _generate_optimizer_class_with_gradient_clipping(\n",
        "        type(optimizer), grad_clipper\n",
        "    )\n",
        "    optimizer.__class__ = OptimizerWithGradientClip\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def build_optimizer(cfg: CfgNode, model: torch.nn.Module) -> torch.optim.Optimizer:\n",
        "    \"\"\"\n",
        "    Build an optimizer from config.\n",
        "    \"\"\"\n",
        "    norm_module_types = (\n",
        "        torch.nn.BatchNorm1d,\n",
        "        torch.nn.BatchNorm2d,\n",
        "        torch.nn.BatchNorm3d,\n",
        "        torch.nn.SyncBatchNorm,\n",
        "        # NaiveSyncBatchNorm inherits from BatchNorm2d\n",
        "        torch.nn.GroupNorm,\n",
        "        torch.nn.InstanceNorm1d,\n",
        "        torch.nn.InstanceNorm2d,\n",
        "        torch.nn.InstanceNorm3d,\n",
        "        torch.nn.LayerNorm,\n",
        "        torch.nn.LocalResponseNorm,\n",
        "    )\n",
        "    params: List[Dict[str, Any]] = []\n",
        "    memo: Set[torch.nn.parameter.Parameter] = set()\n",
        "    for module in model.modules():\n",
        "        for key, value in module.named_parameters(): #recurse=False):\n",
        "            if not value.requires_grad:\n",
        "                continue\n",
        "            # Avoid duplicating parameters\n",
        "            if value in memo:\n",
        "                continue\n",
        "            memo.add(value)\n",
        "            lr = cfg.SOLVER.BASE_LR\n",
        "            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n",
        "            if isinstance(module, norm_module_types):\n",
        "                weight_decay = cfg.SOLVER.WEIGHT_DECAY_NORM\n",
        "            elif \"bias\" in key: #key == \"bias\":\n",
        "                # NOTE: unlike Detectron v1, we now default BIAS_LR_FACTOR to 1.0\n",
        "                # and WEIGHT_DECAY_BIAS to WEIGHT_DECAY so that bias optimizer\n",
        "                # hyperparameters are by default exactly the same as for regular\n",
        "                # weights.\n",
        "                lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR\n",
        "                weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n",
        "\n",
        "            if 'box_predictor' in key:\n",
        "                lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.HEAD_LR_FACTOR\n",
        "            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM, nesterov=cfg.SOLVER.NESTEROV\n",
        "    )\n",
        "    optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def build_lr_scheduler(\n",
        "    cfg: CfgNode, optimizer: torch.optim.Optimizer\n",
        ") -> torch.optim.lr_scheduler._LRScheduler:\n",
        "    \"\"\"\n",
        "    Build a LR scheduler from config.\n",
        "    \"\"\"\n",
        "    name = cfg.SOLVER.LR_SCHEDULER_NAME\n",
        "    if name == \"WarmupMultiStepLR\":\n",
        "        return WarmupMultiStepLR(\n",
        "            optimizer,\n",
        "            cfg.SOLVER.STEPS,\n",
        "            cfg.SOLVER.GAMMA,\n",
        "            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n",
        "            warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n",
        "            warmup_method=cfg.SOLVER.WARMUP_METHOD,\n",
        "        )\n",
        "    elif name == \"WarmupCosineLR\":\n",
        "        return WarmupCosineLR(\n",
        "            optimizer,\n",
        "            cfg.SOLVER.MAX_ITER,\n",
        "            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n",
        "            warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n",
        "            warmup_method=cfg.SOLVER.WARMUP_METHOD,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unknown LR scheduler: {}\".format(name))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing d.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRkkevwoh1h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51c82b2-76ab-4f34-bb78-bf74097dd8cb"
      },
      "source": [
        "%%writefile e.py\n",
        "import contextlib\n",
        "import copy\n",
        "import io\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import pycocotools.mask as mask_util\n",
        "import torch\n",
        "from fvcore.common.file_io import PathManager\n",
        "from pycocotools.coco import COCO\n",
        "from tabulate import tabulate\n",
        "\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.data.datasets.coco import convert_to_coco_json\n",
        "from detectron2.evaluation.fast_eval_api import COCOeval_opt as COCOeval\n",
        "from detectron2.structures import Boxes, BoxMode, pairwise_iou\n",
        "from detectron2.utils.logger import create_small_table\n",
        "\n",
        "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"airplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "    \"chair\", \"cow\", \"dining table\", \"dog\", \"horse\", \"motorcycle\", \"person\",\n",
        "    \"potted plant\", \"sheep\", \"couch\", \"train\", \"tv\",\n",
        "]\n",
        "\n",
        "class COCOEvaluator(DatasetEvaluator):\n",
        "    \"\"\"\n",
        "    Evaluate AR for object proposals, AP for instance detection/segmentation, AP\n",
        "    for keypoint detection outputs using COCO's metrics.\n",
        "    See http://cocodataset.org/#detection-eval and\n",
        "    http://cocodataset.org/#keypoints-eval to understand its metrics.\n",
        "    In addition to COCO, this evaluator is able to support any bounding box detection,\n",
        "    instance segmentation, or keypoint detection dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name, cfg, distributed, output_dir=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_name (str): name of the dataset to be evaluated.\n",
        "                It must have either the following corresponding metadata:\n",
        "                    \"json_file\": the path to the COCO format annotation\n",
        "                Or it must be in detectron2's standard dataset format\n",
        "                so it can be converted to COCO format automatically.\n",
        "            cfg (CfgNode): config instance\n",
        "            distributed (True): if True, will collect results from all ranks and run evaluation\n",
        "                in the main process.\n",
        "                Otherwise, will evaluate the results in the current process.\n",
        "            output_dir (str): optional, an output directory to dump all\n",
        "                results predicted on the dataset. The dump contains two files:\n",
        "                1. \"instance_predictions.pth\" a file in torch serialization\n",
        "                   format that contains all the raw original predictions.\n",
        "                2. \"coco_instances_results.json\" a json file in COCO's result\n",
        "                   format.\n",
        "        \"\"\"\n",
        "        self._tasks = self._tasks_from_config(cfg)\n",
        "        self._distributed = distributed\n",
        "        self._output_dir = output_dir\n",
        "\n",
        "        self._cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "        # replace fewx with d2\n",
        "        self._logger = logging.getLogger(__name__)\n",
        "        self._metadata = MetadataCatalog.get(dataset_name)\n",
        "        if not hasattr(self._metadata, \"json_file\"):\n",
        "            self._logger.info(\n",
        "                f\"'{dataset_name}' is not registered by `register_coco_instances`.\"\n",
        "                \" Therefore trying to convert it to COCO format ...\"\n",
        "            )\n",
        "\n",
        "            cache_path = os.path.join(output_dir, f\"{dataset_name}_coco_format.json\")\n",
        "            self._metadata.json_file = cache_path\n",
        "            convert_to_coco_json(dataset_name, cache_path)\n",
        "\n",
        "        json_file = PathManager.get_local_path(self._metadata.json_file)\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            self._coco_api = COCO(json_file)\n",
        "\n",
        "        self._kpt_oks_sigmas = cfg.TEST.KEYPOINT_OKS_SIGMAS\n",
        "        # Test set json files do not contain annotations (evaluation must be\n",
        "        # performed using the COCO evaluation server).\n",
        "        self._do_evaluation = \"annotations\" in self._coco_api.dataset\n",
        "\n",
        "    def reset(self):\n",
        "        self._predictions = []\n",
        "\n",
        "    def _tasks_from_config(self, cfg):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            tuple[str]: tasks that can be evaluated under the given configuration.\n",
        "        \"\"\"\n",
        "        tasks = (\"bbox\",)\n",
        "        if cfg.MODEL.MASK_ON:\n",
        "            tasks = tasks + (\"segm\",)\n",
        "        if cfg.MODEL.KEYPOINT_ON:\n",
        "            tasks = tasks + (\"keypoints\",)\n",
        "        return tasks\n",
        "\n",
        "    def process(self, inputs, outputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n",
        "                It is a list of dict. Each dict corresponds to an image and\n",
        "                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n",
        "            outputs: the outputs of a COCO model. It is a list of dicts with key\n",
        "                \"instances\" that contains :class:`Instances`.\n",
        "        \"\"\"\n",
        "        for input, output in zip(inputs, outputs):\n",
        "            prediction = {\"image_id\": input[\"image_id\"]}\n",
        "\n",
        "            # TODO this is ugly\n",
        "            if \"instances\" in output:\n",
        "                instances = output[\"instances\"].to(self._cpu_device)\n",
        "                prediction[\"instances\"] = instances_to_coco_json(instances, input[\"image_id\"])\n",
        "            if \"proposals\" in output:\n",
        "                prediction[\"proposals\"] = output[\"proposals\"].to(self._cpu_device)\n",
        "            self._predictions.append(prediction)\n",
        "\n",
        "    def evaluate(self):\n",
        "        if self._distributed:\n",
        "            comm.synchronize()\n",
        "            predictions = comm.gather(self._predictions, dst=0)\n",
        "            predictions = list(itertools.chain(*predictions))\n",
        "\n",
        "            if not comm.is_main_process():\n",
        "                return {}\n",
        "        else:\n",
        "            predictions = self._predictions\n",
        "\n",
        "        if len(predictions) == 0:\n",
        "            self._logger.warning(\"[COCOEvaluator] Did not receive valid predictions.\")\n",
        "            return {}\n",
        "\n",
        "        if self._output_dir:\n",
        "            PathManager.mkdirs(self._output_dir)\n",
        "            file_path = os.path.join(self._output_dir, \"instances_predictions.pth\")\n",
        "            with PathManager.open(file_path, \"wb\") as f:\n",
        "                torch.save(predictions, f)\n",
        "\n",
        "        self._results = OrderedDict()\n",
        "        if \"proposals\" in predictions[0]:\n",
        "            self._eval_box_proposals(predictions)\n",
        "        if \"instances\" in predictions[0]:\n",
        "            self._eval_predictions(set(self._tasks), predictions)\n",
        "        # Copy so the caller can do whatever with results\n",
        "        return copy.deepcopy(self._results)\n",
        "\n",
        "    def _eval_predictions(self, tasks, predictions):\n",
        "        \"\"\"\n",
        "        Evaluate predictions on the given tasks.\n",
        "        Fill self._results with the metrics of the tasks.\n",
        "        \"\"\"\n",
        "        self._logger.info(\"Preparing results for COCO format ...\")\n",
        "        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n",
        "\n",
        "        # unmap the category ids for COCO\n",
        "        if hasattr(self._metadata, \"thing_dataset_id_to_contiguous_id\"):\n",
        "            reverse_id_mapping = {\n",
        "                v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()\n",
        "            }\n",
        "            for result in coco_results:\n",
        "                category_id = result[\"category_id\"]\n",
        "                assert (\n",
        "                    category_id in reverse_id_mapping\n",
        "                ), \"A prediction has category_id={}, which is not available in the dataset.\".format(\n",
        "                    category_id\n",
        "                )\n",
        "                result[\"category_id\"] = reverse_id_mapping[category_id]\n",
        "\n",
        "        if self._output_dir:\n",
        "            file_path = os.path.join(self._output_dir, \"coco_instances_results.json\")\n",
        "            self._logger.info(\"Saving results to {}\".format(file_path))\n",
        "            with PathManager.open(file_path, \"w\") as f:\n",
        "                f.write(json.dumps(coco_results))\n",
        "                f.flush()\n",
        "\n",
        "        if not self._do_evaluation:\n",
        "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
        "            return\n",
        "\n",
        "        self._logger.info(\"Evaluating predictions ...\")\n",
        "        for task in sorted(tasks):\n",
        "            coco_eval = (\n",
        "                _evaluate_predictions_on_coco(\n",
        "                    self._coco_api, coco_results, task, kpt_oks_sigmas=self._kpt_oks_sigmas\n",
        "                )\n",
        "                if len(coco_results) > 0\n",
        "                else None  # cocoapi does not handle empty results very well\n",
        "            )\n",
        "\n",
        "            res = self._derive_coco_results(\n",
        "                coco_eval, task, class_names=self._metadata.get(\"thing_classes\")\n",
        "            )\n",
        "            self._results[task] = res\n",
        "\n",
        "    def _eval_box_proposals(self, predictions):\n",
        "        \"\"\"\n",
        "        Evaluate the box proposals in predictions.\n",
        "        Fill self._results with the metrics for \"box_proposals\" task.\n",
        "        \"\"\"\n",
        "        if self._output_dir:\n",
        "            # Saving generated box proposals to file.\n",
        "            # Predicted box_proposals are in XYXY_ABS mode.\n",
        "            bbox_mode = BoxMode.XYXY_ABS.value\n",
        "            ids, boxes, objectness_logits = [], [], []\n",
        "            for prediction in predictions:\n",
        "                ids.append(prediction[\"image_id\"])\n",
        "                boxes.append(prediction[\"proposals\"].proposal_boxes.tensor.numpy())\n",
        "                objectness_logits.append(prediction[\"proposals\"].objectness_logits.numpy())\n",
        "\n",
        "            proposal_data = {\n",
        "                \"boxes\": boxes,\n",
        "                \"objectness_logits\": objectness_logits,\n",
        "                \"ids\": ids,\n",
        "                \"bbox_mode\": bbox_mode,\n",
        "            }\n",
        "            with PathManager.open(os.path.join(self._output_dir, \"box_proposals.pkl\"), \"wb\") as f:\n",
        "                pickle.dump(proposal_data, f)\n",
        "\n",
        "        if not self._do_evaluation:\n",
        "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
        "            return\n",
        "\n",
        "        self._logger.info(\"Evaluating bbox proposals ...\")\n",
        "        res = {}\n",
        "        areas = {\"all\": \"\", \"small\": \"s\", \"medium\": \"m\", \"large\": \"l\"}\n",
        "        for limit in [100, 1000]:\n",
        "            for area, suffix in areas.items():\n",
        "                stats = _evaluate_box_proposals(predictions, self._coco_api, area=area, limit=limit)\n",
        "                key = \"AR{}@{:d}\".format(suffix, limit)\n",
        "                res[key] = float(stats[\"ar\"].item() * 100)\n",
        "        self._logger.info(\"Proposal metrics: \\n\" + create_small_table(res))\n",
        "        self._results[\"box_proposals\"] = res\n",
        "\n",
        "    def _calculate_ap(self, class_names, precisions, T=None, A=None):\n",
        "        ################## ap #####################\n",
        "        voc_ls = []\n",
        "        non_voc_ls = []\n",
        "\n",
        "        for idx, name in enumerate(class_names):\n",
        "            # area range index 0: all area ranges\n",
        "            # max dets index -1: typically 100 per image\n",
        "            if T is not None and A is None:\n",
        "                precision = precisions[T, :, idx, 0, -1]\n",
        "            elif A is not None and T is None:\n",
        "                precision = precisions[:, :, idx, A, -1]\n",
        "            elif T is None and A is None:\n",
        "                precision = precisions[:, :, idx, 0, -1]\n",
        "            else:\n",
        "                assert False\n",
        "\n",
        "            precision = precision[precision > -1]\n",
        "            ap = np.mean(precision) if precision.size else float(\"nan\")\n",
        "\n",
        "            # calculate voc ap and non-voc ap\n",
        "            if name in CLASS_NAMES:\n",
        "                voc_ls.append(ap * 100)\n",
        "            else:\n",
        "                non_voc_ls.append(ap * 100)\n",
        "        if len(voc_ls) > 0:\n",
        "            voc_ap = sum(voc_ls) * 1.0 / len(voc_ls)\n",
        "        else:\n",
        "            voc_ap = 0.0\n",
        "        if len(non_voc_ls) > 0:\n",
        "            non_voc_ap = sum(non_voc_ls) * 1.0 / len(non_voc_ls)\n",
        "        else:\n",
        "            non_voc_ap = 0.0\n",
        "\n",
        "        return voc_ap, non_voc_ap\n",
        "\n",
        "    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n",
        "        \"\"\"\n",
        "        Derive the desired score numbers from summarized COCOeval.\n",
        "        Args:\n",
        "            coco_eval (None or COCOEval): None represents no predictions from model.\n",
        "            iou_type (str):\n",
        "            class_names (None or list[str]): if provided, will use it to predict\n",
        "                per-category AP.\n",
        "        Returns:\n",
        "            a dict of {metric name: score}\n",
        "        \"\"\"\n",
        "\n",
        "        metrics = {\n",
        "            \"bbox\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
        "            \"segm\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
        "            \"keypoints\": [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"],\n",
        "        }[iou_type]\n",
        "\n",
        "        if coco_eval is None:\n",
        "            self._logger.warn(\"No predictions from the model!\")\n",
        "            return {metric: float(\"nan\") for metric in metrics}\n",
        "\n",
        "        # the standard metrics\n",
        "        results = {\n",
        "            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else \"nan\")\n",
        "            for idx, metric in enumerate(metrics)\n",
        "        }\n",
        "        self._logger.info(\n",
        "            \"Evaluation results for {}: \\n\".format(iou_type) + create_small_table(results)\n",
        "        )\n",
        "        if not np.isfinite(sum(results.values())):\n",
        "            self._logger.info(\"Some metrics cannot be computed and is shown as NaN.\")\n",
        "\n",
        "        if class_names is None or len(class_names) <= 1:\n",
        "            return results\n",
        "        # Compute per-category AP\n",
        "        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa\n",
        "        precisions = coco_eval.eval[\"precision\"]\n",
        "        # precision has dims (iou, recall, cls, area range, max dets)\n",
        "        assert len(class_names) == precisions.shape[2]\n",
        "\n",
        "        results_per_category = []\n",
        "        voc_ls = []\n",
        "        non_voc_ls = []\n",
        "\n",
        "        ################## ap #####################\n",
        "        for idx, name in enumerate(class_names):\n",
        "            # area range index 0: all area ranges\n",
        "            # max dets index -1: typically 100 per image\n",
        "            precision = precisions[:, :, idx, 0, -1]\n",
        "            precision = precision[precision > -1]\n",
        "            ap = np.mean(precision) if precision.size else float(\"nan\")\n",
        "            results_per_category.append((\"{}\".format(name), float(ap * 100)))\n",
        "        # calculate voc and non voc metrics\n",
        "        voc_ap, non_voc_ap = self._calculate_ap(class_names, precisions)\n",
        "        voc_ap_50, non_voc_ap_50 = self._calculate_ap(class_names, precisions, T=0) # T=0, iou_thresh = 0.5\n",
        "        voc_ap_75, non_voc_ap_75 = self._calculate_ap(class_names, precisions, T=5) # T=5, iou_thresh = 0.75\n",
        "\n",
        "        voc_ap_small, non_voc_ap_small = self._calculate_ap(class_names, precisions, A=1) # A=1, small\n",
        "        voc_ap_medium, non_voc_ap_medium = self._calculate_ap(class_names, precisions, A=2) # A=2, medium\n",
        "        voc_ap_large, non_voc_ap_large = self._calculate_ap(class_names, precisions, A=3) # A=3, large\n",
        "\n",
        "        # print voc ap\n",
        "        self._logger.info(\"Evaluation results for VOC 20 categories =======> AP  : \" + str('%.2f' % voc_ap))\n",
        "        self._logger.info(\"Evaluation results for VOC 20 categories =======> AP50: \" + str('%.2f' % voc_ap_50))\n",
        "        self._logger.info(\"Evaluation results for VOC 20 categories =======> AP75: \" + str('%.2f' % voc_ap_75))\n",
        "        self._logger.info(\"Evaluation results for VOC 20 categories =======> APs : \" + str('%.2f' % voc_ap_small))\n",
        "        self._logger.info(\"Evaluation results for VOC 20 categories =======> APm : \" + str('%.2f' % voc_ap_medium))\n",
        "        self._logger.info(\"Evaluation results for VOC 20 categories =======> APl : \" + str('%.2f' % voc_ap_large))\n",
        "\n",
        "        # print voc ap\n",
        "        self._logger.info(\"Evaluation results for Non VOC 60 categories =======> AP  : \" + str('%.2f' % non_voc_ap))\n",
        "        self._logger.info(\"Evaluation results for Non VOC 60 categories =======> AP50: \" + str('%.2f' % non_voc_ap_50))\n",
        "        self._logger.info(\"Evaluation results for Non VOC 60 categories =======> AP75: \" + str('%.2f' % non_voc_ap_75))\n",
        "        self._logger.info(\"Evaluation results for Non VOC 60 categories =======> APs : \" + str('%.2f' % non_voc_ap_small))\n",
        "        self._logger.info(\"Evaluation results for Non VOC 60 categories =======> APm : \" + str('%.2f' % non_voc_ap_medium))\n",
        "        self._logger.info(\"Evaluation results for Non VOC 60 categories =======> APl : \" + str('%.2f' % non_voc_ap_large))\n",
        "\n",
        "        '''\n",
        "        # log evaluation results in csv\n",
        "        # type, AP, AP50, AP75, APs, APm, APl\n",
        "        eval_log = iou_type + ',' + 'AP,AP50,AP75,APs,APm,APl' + '\\n'\n",
        "        eval_log += 'all' + ',' + str('%.2f' % results['AP']) + ',' + str('%.2f' % results['AP50']) + ',' + str('%.2f' % results['AP75']) + ',' + str('%.2f' % results['APs']) + ',' + str('%.2f' % results['APm']) + ',' + str('%.2f' % results['APl']) + '\\n'\n",
        "        eval_log += 'VOC' + ',' + str('%.2f' % voc_ap) + ',' + str('%.2f' % voc_ap_50) + ',' + str('%.2f' % voc_ap_75) + ',' + str('%.2f' % voc_ap_small) + ',' + str('%.2f' % voc_ap_medium) + ',' + str('%.2f' % voc_ap_large) + '\\n'\n",
        "        eval_log += 'Non-VOC' + ',' + str('%.2f' % non_voc_ap) + ',' + str('%.2f' % non_voc_ap_50) + ',' + str('%.2f' % non_voc_ap_75) + ',' + str('%.2f' % non_voc_ap_small) + ',' + str('%.2f' % non_voc_ap_medium) + ',' + str('%.2f' % non_voc_ap_large) + '\\n'\n",
        "        with open('evaluation_result.csv', 'a') as f:\n",
        "            f.write(eval_log)\n",
        "        '''\n",
        "        # tabulate it\n",
        "        N_COLS = min(6, len(results_per_category) * 2)\n",
        "        results_flatten = list(itertools.chain(*results_per_category))\n",
        "        results_2d = itertools.zip_longest(*[results_flatten[i::N_COLS] for i in range(N_COLS)])\n",
        "        table = tabulate(\n",
        "            results_2d,\n",
        "            tablefmt=\"pipe\",\n",
        "            floatfmt=\".3f\",\n",
        "            headers=[\"category\", \"AP\"] * (N_COLS // 2),\n",
        "            numalign=\"left\",\n",
        "        )\n",
        "        self._logger.info(\"Per-category {} AP: \\n\".format(iou_type) + table)\n",
        "\n",
        "        results.update({\"AP-\" + name: ap for name, ap in results_per_category})\n",
        "        return results\n",
        "\n",
        "def instances_to_coco_json(instances, img_id):\n",
        "    \"\"\"\n",
        "    Dump an \"Instances\" object to a COCO-format json that's used for evaluation.\n",
        "    Args:\n",
        "        instances (Instances):\n",
        "        img_id (int): the image id\n",
        "    Returns:\n",
        "        list[dict]: list of json annotations in COCO format.\n",
        "    \"\"\"\n",
        "    num_instance = len(instances)\n",
        "    if num_instance == 0:\n",
        "        return []\n",
        "\n",
        "    boxes = instances.pred_boxes.tensor.numpy()\n",
        "    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n",
        "    boxes = boxes.tolist()\n",
        "    scores = instances.scores.tolist()\n",
        "    classes = instances.pred_classes.tolist()\n",
        "\n",
        "    has_mask = instances.has(\"pred_masks\")\n",
        "    if has_mask:\n",
        "        # use RLE to encode the masks, because they are too large and takes memory\n",
        "        # since this evaluator stores outputs of the entire dataset\n",
        "        rles = [\n",
        "            mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n",
        "            for mask in instances.pred_masks\n",
        "        ]\n",
        "        for rle in rles:\n",
        "            # \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\n",
        "            # json writer which always produces strings cannot serialize a bytestream\n",
        "            # unless you decode it. Thankfully, utf-8 works out (which is also what\n",
        "            # the pycocotools/_mask.pyx does).\n",
        "            rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "    has_keypoints = instances.has(\"pred_keypoints\")\n",
        "    if has_keypoints:\n",
        "        keypoints = instances.pred_keypoints\n",
        "\n",
        "    results = []\n",
        "    for k in range(num_instance):\n",
        "        result = {\n",
        "            \"image_id\": img_id,\n",
        "            \"category_id\": classes[k],\n",
        "            \"bbox\": boxes[k],\n",
        "            \"score\": scores[k],\n",
        "        }\n",
        "        if has_mask:\n",
        "            result[\"segmentation\"] = rles[k]\n",
        "        if has_keypoints:\n",
        "            # In COCO annotations,\n",
        "            # keypoints coordinates are pixel indices.\n",
        "            # However our predictions are floating point coordinates.\n",
        "            # Therefore we subtract 0.5 to be consistent with the annotation format.\n",
        "            # This is the inverse of data loading logic in `datasets/coco.py`.\n",
        "            keypoints[k][:, :2] -= 0.5\n",
        "            result[\"keypoints\"] = keypoints[k].flatten().tolist()\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "\n",
        "# inspired from Detectron:\n",
        "# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa\n",
        "def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=\"all\", limit=None):\n",
        "    \"\"\"\n",
        "    Evaluate detection proposal recall metrics. This function is a much\n",
        "    faster alternative to the official COCO API recall evaluation code. However,\n",
        "    it produces slightly different results.\n",
        "    \"\"\"\n",
        "    # Record max overlap value for each gt box\n",
        "    # Return vector of overlap values\n",
        "    areas = {\n",
        "        \"all\": 0,\n",
        "        \"small\": 1,\n",
        "        \"medium\": 2,\n",
        "        \"large\": 3,\n",
        "        \"96-128\": 4,\n",
        "        \"128-256\": 5,\n",
        "        \"256-512\": 6,\n",
        "        \"512-inf\": 7,\n",
        "    }\n",
        "    area_ranges = [\n",
        "        [0 ** 2, 1e5 ** 2],  # all\n",
        "        [0 ** 2, 32 ** 2],  # small\n",
        "        [32 ** 2, 96 ** 2],  # medium\n",
        "        [96 ** 2, 1e5 ** 2],  # large\n",
        "        [96 ** 2, 128 ** 2],  # 96-128\n",
        "        [128 ** 2, 256 ** 2],  # 128-256\n",
        "        [256 ** 2, 512 ** 2],  # 256-512\n",
        "        [512 ** 2, 1e5 ** 2],\n",
        "    ]  # 512-inf\n",
        "    assert area in areas, \"Unknown area range: {}\".format(area)\n",
        "    area_range = area_ranges[areas[area]]\n",
        "    gt_overlaps = []\n",
        "    num_pos = 0\n",
        "\n",
        "    for prediction_dict in dataset_predictions:\n",
        "        predictions = prediction_dict[\"proposals\"]\n",
        "\n",
        "        # sort predictions in descending order\n",
        "        # TODO maybe remove this and make it explicit in the documentation\n",
        "        inds = predictions.objectness_logits.sort(descending=True)[1]\n",
        "        predictions = predictions[inds]\n",
        "\n",
        "        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[\"image_id\"])\n",
        "        anno = coco_api.loadAnns(ann_ids)\n",
        "        gt_boxes = [\n",
        "            BoxMode.convert(obj[\"bbox\"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
        "            for obj in anno\n",
        "            if obj[\"iscrowd\"] == 0\n",
        "        ]\n",
        "        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes\n",
        "        gt_boxes = Boxes(gt_boxes)\n",
        "        gt_areas = torch.as_tensor([obj[\"area\"] for obj in anno if obj[\"iscrowd\"] == 0])\n",
        "\n",
        "        if len(gt_boxes) == 0 or len(predictions) == 0:\n",
        "            continue\n",
        "\n",
        "        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n",
        "        gt_boxes = gt_boxes[valid_gt_inds]\n",
        "\n",
        "        num_pos += len(gt_boxes)\n",
        "\n",
        "        if len(gt_boxes) == 0:\n",
        "            continue\n",
        "\n",
        "        if limit is not None and len(predictions) > limit:\n",
        "            predictions = predictions[:limit]\n",
        "\n",
        "        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)\n",
        "\n",
        "        _gt_overlaps = torch.zeros(len(gt_boxes))\n",
        "        for j in range(min(len(predictions), len(gt_boxes))):\n",
        "            # find which proposal box maximally covers each gt box\n",
        "            # and get the iou amount of coverage for each gt box\n",
        "            max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n",
        "\n",
        "            # find which gt box is 'best' covered (i.e. 'best' = most iou)\n",
        "            gt_ovr, gt_ind = max_overlaps.max(dim=0)\n",
        "            assert gt_ovr >= 0\n",
        "            # find the proposal box that covers the best covered gt box\n",
        "            box_ind = argmax_overlaps[gt_ind]\n",
        "            # record the iou coverage of this gt box\n",
        "            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n",
        "            assert _gt_overlaps[j] == gt_ovr\n",
        "            # mark the proposal box and the gt box as used\n",
        "            overlaps[box_ind, :] = -1\n",
        "            overlaps[:, gt_ind] = -1\n",
        "\n",
        "        # append recorded iou coverage level\n",
        "        gt_overlaps.append(_gt_overlaps)\n",
        "    gt_overlaps = (\n",
        "        torch.cat(gt_overlaps, dim=0) if len(gt_overlaps) else torch.zeros(0, dtype=torch.float32)\n",
        "    )\n",
        "    gt_overlaps, _ = torch.sort(gt_overlaps)\n",
        "\n",
        "    if thresholds is None:\n",
        "        step = 0.05\n",
        "        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)\n",
        "    recalls = torch.zeros_like(thresholds)\n",
        "    # compute recall for each iou threshold\n",
        "    for i, t in enumerate(thresholds):\n",
        "        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)\n",
        "    # ar = 2 * np.trapz(recalls, thresholds)\n",
        "    ar = recalls.mean()\n",
        "    return {\n",
        "        \"ar\": ar,\n",
        "        \"recalls\": recalls,\n",
        "        \"thresholds\": thresholds,\n",
        "        \"gt_overlaps\": gt_overlaps,\n",
        "        \"num_pos\": num_pos,\n",
        "    }\n",
        "\n",
        "\n",
        "def _evaluate_predictions_on_coco(coco_gt, coco_results, iou_type, kpt_oks_sigmas=None):\n",
        "    \"\"\"\n",
        "    Evaluate the coco results using COCOEval API.\n",
        "    \"\"\"\n",
        "    assert len(coco_results) > 0\n",
        "\n",
        "    if iou_type == \"segm\":\n",
        "        coco_results = copy.deepcopy(coco_results)\n",
        "        # When evaluating mask AP, if the results contain bbox, cocoapi will\n",
        "        # use the box area as the area of the instance, instead of the mask area.\n",
        "        # This leads to a different definition of small/medium/large.\n",
        "        # We remove the bbox field to let mask AP use mask area.\n",
        "        for c in coco_results:\n",
        "            c.pop(\"bbox\", None)\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(coco_results)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)\n",
        "\n",
        "    if iou_type == \"keypoints\":\n",
        "        # Use the COCO default keypoint OKS sigmas unless overrides are specified\n",
        "        if kpt_oks_sigmas:\n",
        "            assert hasattr(coco_eval.params, \"kpt_oks_sigmas\"), \"pycocotools is too old!\"\n",
        "            coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)\n",
        "        # COCOAPI requires every detection and every gt to have keypoints, so\n",
        "        # we just take the first entry from both\n",
        "        num_keypoints_dt = len(coco_results[0][\"keypoints\"]) // 3\n",
        "        num_keypoints_gt = len(next(iter(coco_gt.anns.values()))[\"keypoints\"]) // 3\n",
        "        num_keypoints_oks = len(coco_eval.params.kpt_oks_sigmas)\n",
        "        assert num_keypoints_oks == num_keypoints_dt == num_keypoints_gt, (\n",
        "            f\"[COCOEvaluator] Prediction contain {num_keypoints_dt} keypoints. \"\n",
        "            f\"Ground truth contains {num_keypoints_gt} keypoints. \"\n",
        "            f\"The length of cfg.TEST.KEYPOINT_OKS_SIGMAS is {num_keypoints_oks}. \"\n",
        "            \"They have to agree with each other. For meaning of OKS, please refer to \"\n",
        "            \"http://cocodataset.org/#keypoints-eval.\"\n",
        "        )\n",
        "\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing e.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-0agQjyeApn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b332da9-ef9e-4850-ad63-30c006b5fdac"
      },
      "source": [
        "%%writefile fsod_train_net.py\n",
        "import os\n",
        "\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, launch\n",
        "#from detectron2.evaluation import COCOEvaluator\n",
        "#from detectron2.data import build_detection_train_loader\n",
        "from detectron2.data import build_batch_data_loader\n",
        "\n",
        "from a import get_cfg\n",
        "from b import DatasetMapperWithSupport\n",
        "from c import build_detection_train_loader, build_detection_test_loader\n",
        "from d import build_optimizer\n",
        "from e import COCOEvaluator\n",
        "\n",
        "import bisect\n",
        "import copy\n",
        "import itertools\n",
        "import logging\n",
        "import numpy as np\n",
        "import operator\n",
        "import pickle\n",
        "import torch.utils.data\n",
        "\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "\n",
        "class Trainer(DefaultTrainer):\n",
        "\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            iterable\n",
        "        It calls :func:`detectron2.data.build_detection_train_loader` with a customized\n",
        "        DatasetMapper, which adds categorical labels as a semantic mask.\n",
        "        \"\"\"\n",
        "        mapper = DatasetMapperWithSupport(cfg)\n",
        "        return build_detection_train_loader(cfg, mapper)\n",
        "\n",
        "    @classmethod\n",
        "    def build_test_loader(cls, cfg, dataset_name):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            iterable\n",
        "        It now calls :func:`detectron2.data.build_detection_test_loader`.\n",
        "        Overwrite it if you'd like a different data loader.\n",
        "        \"\"\"\n",
        "        return build_detection_test_loader(cfg, dataset_name)\n",
        "\n",
        "    @classmethod\n",
        "    def build_optimizer(cls, cfg, model):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            torch.optim.Optimizer:\n",
        "        It now calls :func:`detectron2.solver.build_optimizer`.\n",
        "        Overwrite it if you'd like a different optimizer.\n",
        "        \"\"\"\n",
        "        return build_optimizer(cfg, model)\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "\n",
        "\n",
        "def setup(args):\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(args.config_file)\n",
        "    cfg.merge_from_list(args.opts)\n",
        "    cfg.freeze()\n",
        "    default_setup(cfg, args)\n",
        "\n",
        "    rank = comm.get_rank()\n",
        "    setup_logger(cfg.OUTPUT_DIR, distributed_rank=rank, name=\"fewx\")\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    cfg = setup(args)\n",
        "\n",
        "    if args.eval_only:\n",
        "        model = Trainer.build_model(cfg)\n",
        "        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
        "            cfg.MODEL.WEIGHTS, resume=args.resume\n",
        "        )\n",
        "        res = Trainer.test(cfg, model)\n",
        "        return res\n",
        "\n",
        "    trainer = Trainer(cfg)\n",
        "    trainer.resume_or_load(resume=args.resume)\n",
        "    return trainer.train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = default_argument_parser().parse_args()\n",
        "    print(args)\n",
        "    print(\"Command Line Args:\", args)\n",
        "    launch(\n",
        "        main,\n",
        "        args.num_gpus,\n",
        "        num_machines=args.num_machines,\n",
        "        machine_rank=args.machine_rank,\n",
        "        dist_url=args.dist_url,\n",
        "        args=(args,),\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing fsod_train_net.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytoXNZgfdhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5396534f-6598-4358-94ea-e1f40f828c17"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python3 fsod_train_net.py --num-gpus 1 \\\n",
        "\t--config-file /content/FewX/configs/fsod/R_50_C4_1x.yaml 2>&1 | tee log/fsod_train_log.txt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tee: log/fsod_train_log.txt: No such file or directory\n",
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Namespace(config_file='/content/FewX/configs/fsod/R_50_C4_1x.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)\n",
            "Command Line Args: Namespace(config_file='/content/FewX/configs/fsod/R_50_C4_1x.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)\n",
            "\u001b[32m[03/25 10:17:28 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\u001b[32m[03/25 10:17:28 detectron2]: \u001b[0mEnvironment info:\n",
            "---------------------  ---------------------------------------------------------------\n",
            "sys.platform           linux\n",
            "Python                 3.7.10 (default, Feb 20 2021, 21:17:23) [GCC 7.5.0]\n",
            "numpy                  1.19.5\n",
            "detectron2             0.4 @/usr/local/lib/python3.7/dist-packages/detectron2\n",
            "Compiler               GCC 7.5\n",
            "CUDA compiler          not available\n",
            "DETECTRON2_ENV_MODULE  <not set>\n",
            "PyTorch                1.8.0+cu101 @/usr/local/lib/python3.7/dist-packages/torch\n",
            "PyTorch debug build    False\n",
            "GPU available          False\n",
            "Pillow                 8.1.2\n",
            "torchvision            0.9.0+cu101 @/usr/local/lib/python3.7/dist-packages/torchvision\n",
            "fvcore                 0.1.4.post20210325\n",
            "cv2                    4.1.2\n",
            "---------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[03/25 10:17:28 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/FewX/configs/fsod/R_50_C4_1x.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)\n",
            "\u001b[32m[03/25 10:17:28 detectron2]: \u001b[0mContents of args.config_file=/content/FewX/configs/fsod/R_50_C4_1x.yaml:\n",
            "_BASE_: \"Base-FSOD-C4.yaml\"\n",
            "MODEL:\n",
            "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
            "  MASK_ON: False\n",
            "  RESNETS:\n",
            "    DEPTH: 50\n",
            "OUTPUT_DIR: './output/fsod/R_50_C4_1x'\n",
            "\n",
            "\u001b[32m[03/25 10:17:28 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 8\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train_nonvoc',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  FS:\n",
            "    FEW_SHOT: False\n",
            "    SUPPORT_SHOT: 10\n",
            "    SUPPORT_WAY: 2\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1000\n",
            "  MAX_SIZE_TRAIN: 1000\n",
            "  MIN_SIZE_TEST: 600\n",
            "  MIN_SIZE_TRAIN: (440, 472, 504, 536, 568, 600)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 3\n",
            "    NAME: build_resnet_backbone\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: []\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: FsodRCNN\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: FsodRPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res4']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 128\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: FsodRes5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 1\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 100\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl\n",
            "OUTPUT_DIR: ./output/fsod/R_50_C4_1x\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.004\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 30000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  HEAD_LR_FACTOR: 2.0\n",
            "  IMS_PER_BATCH: 8\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 120000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (112000, 120000)\n",
            "  WARMUP_FACTOR: 0.1\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[03/25 10:17:28 detectron2]: \u001b[0mFull config saved to ./output/fsod/R_50_C4_1x/config.yaml\n",
            "\u001b[32m[03/25 10:17:28 d2.utils.env]: \u001b[0mUsing a generated random seed 28875269\n",
            "Traceback (most recent call last):\n",
            "  File \"fsod_train_net.py\", line 110, in <module>\n",
            "    args=(args,),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"fsod_train_net.py\", line 95, in main\n",
            "    trainer = Trainer(cfg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\", line 310, in __init__\n",
            "    model = self.build_model(cfg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\", line 452, in build_model\n",
            "    model = build_model(cfg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/build.py\", line 22, in build_model\n",
            "    model = META_ARCH_REGISTRY.get(meta_arch)(cfg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fvcore/common/registry.py\", line 72, in get\n",
            "    \"No object named '{}' found in '{}' registry!\".format(name, self._name)\n",
            "KeyError: \"No object named 'FsodRCNN' found in 'META_ARCH' registry!\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPtc40iXAr04"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}